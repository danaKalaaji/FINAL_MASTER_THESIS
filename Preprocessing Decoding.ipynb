{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339a2125-a679-43b4-9edd-82d88d45d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import librosa\n",
    "import random\n",
    "from segment_service import align_texts\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e208d5d",
   "metadata": {},
   "source": [
    "# The paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780723e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "recordings_folder_path = r\"../Intervention_T1_data/recordings/deco_trials/FR_deco/FR_deco\"\n",
    "decoding_trial_path = r\"../Intervention_T1_data/ground_truth_transcribed/Decoding/deco_test_ground_truth_FR_T1_first_coder_.csv\"\n",
    "decoding_trial_path_ = r\"../Intervention_T1_data/ground_truth_transcribed/Decoding/deco_test_ground_truth_FR_T1_first_coder.xlsx\"\n",
    "phon_del_trial_path = r\"../Intervention_T1_data/ground_truth_transcribed/PhonDel/phonDel_test_ground_truth_ALL_T1_first_coder.csv\"\n",
    "path_filters = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c8d2f-527c-4547-9803-b979029f52a7",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bca996b",
   "metadata": {},
   "source": [
    "Load the dataframe containing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afd9fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(decoding_trial_path)\n",
    "#read xls file\n",
    "df_ = pd.read_excel(decoding_trial_path_)\n",
    "#df = pd.read_excel(decoding_trial_path_)\n",
    "print(\"Number of rows in df: \", len(df), len(df_))\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e742b56",
   "metadata": {},
   "source": [
    "## Modification of the database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0a28d9",
   "metadata": {},
   "source": [
    "1) Drop some columns that are no relevant\n",
    "    - date_time, timePoint, trial_id\n",
    "2) Change some column names to have more evocative names\n",
    "    - config --> reference_text\n",
    "    - trial_answer --> human_transcription\n",
    "3) Add some column\n",
    "    - words_human_transcription: contains the transcription without the annotations\n",
    "    - human_alignment: contains the alignment between the clinician and asr transcription\n",
    "    - audio_length: duration of corresponding audio\n",
    "    - process_time: time ot took to the ASR to process the audio\n",
    "    - asr_response: response of the ASR\n",
    "    - asr_comparison: comparison between clinician and asr transcription (TP, TN, FP, FN)\n",
    "4) Remove rows of the database without clinician annotations: 104 rows (all were marked as duplicate trial)\n",
    "5) Keep only the files that are present in both the dataframe and in the recording folder and remove duplicate file (duplicate with respect to the file names)\n",
    "6) Replace all ';' by a ' ' in the reference_text column for future alignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61992b3f-4d81-4e2e-af7d-d31dfd0f37a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_df(df, columns_drop, columns_rename, columns_add):\n",
    "    df = df.drop(columns=columns_drop)\n",
    "    df = df.rename(columns=columns_rename)\n",
    "    for col in columns_add:\n",
    "        df[col] = \"\"\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe973cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_files_df_recordings(df, recordings_folder_path, path_filters=None):    \n",
    "    recordings_filenames = []\n",
    "    recordings_filepaths = []\n",
    "    if path_filters is None:\n",
    "        path_filters = ()\n",
    "\n",
    "    # Get list of all audio filenames and paths in recordings folder whose paths contains all path filters\n",
    "    for root, _, files in os.walk(recordings_folder_path):\n",
    "        if all(filter in root for filter in path_filters):\n",
    "            for file in files:\n",
    "                if file.endswith(\".wav\"):\n",
    "                    recordings_filenames.append(file)\n",
    "                    recordings_filepaths.append(os.path.join(root, file))\n",
    "\n",
    "    # Get the list of filenames in the json file\n",
    "    df_filenames = df['filename'].tolist()\n",
    "\n",
    "    # Get the list of filenames in the df file that are not in the recordings folder\n",
    "    missing_recordings = list(set(df_filenames) - set(recordings_filenames))\n",
    "    print(len(missing_recordings), \" Files that are in the df but not in the recording folder (not processed): \", missing_recordings)\n",
    "\n",
    "    # Get the list of filenames in the recordings folder that are not in the df\n",
    "    extra_recordings = list(set(recordings_filenames) - set(df_filenames))\n",
    "    print(len(extra_recordings), \" Files that are in the recording folder but not in the df (not processed): \", extra_recordings)\n",
    "\n",
    "    # Remove extra files from recordings_filepaths and missing files from df\n",
    "    recordings_filepaths = [path for path in recordings_filepaths if os.path.basename(path) not in extra_recordings]\n",
    "    df = df[~df['filename'].isin(missing_recordings)]\n",
    "\n",
    "    # Remove the duplicate filenames from df and from recordings_filepaths\n",
    "    print(\"Duplicate filenames in df (not processed): \", df[df['filename'].duplicated(keep=False)].sort_values('filename')['filename'].tolist())\n",
    "    duplicate_list = [item for item, count in Counter(recordings_filepaths).items() if count > 1]\n",
    "    print(\"Duplicate filenames in recordings folder (not processed): \", [item for item, count in Counter(recordings_filepaths).items() if count > 1])\n",
    "    df = df.drop_duplicates(subset=['filename'], keep='first')\n",
    "    recordings_filepaths = list(set(recordings_filepaths))\n",
    "\n",
    "    # reset index of DataFrame\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Verify number of files in DataFrame matches number of files in folder\n",
    "    assert len(df) == len(recordings_filepaths), f\"Number of files in DataFrame ({len(df)}) does not match number of files in folder ({len(recordings_filepaths)})\"\n",
    "    print(\"Number of files to be processed: \", len(df))\n",
    "\n",
    "    return df, recordings_filepaths, recordings_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0905e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change format of df to match the expected format ###\n",
    "df = reformat_df(df = df, \n",
    "                    columns_drop = ['date_time','timePoint', 'trial_id'], \n",
    "                    columns_rename = {\"config\":\"reference_text\", \"trial_answer\":\"human_transcription\", \"file_name\": \"filename\"}, \n",
    "                    columns_add = [\"words_human_transcription\", \"human_alignment\", \"asr_response\", \"asr_comparison\", \"audio_length\", \"process_time\", \"filepath\", \"accuracy_og\"])\n",
    "# Reorder the DataFrame columns\n",
    "desired_order = ['participant_id', 'session_id', 'language', 'session_form', 'phase',\n",
    "                 'config_id', 'filename', 'reference_text', 'words_human_transcription',\n",
    "                 'human_transcription', 'accuracy', \"accuracy_og\", 'notes', 'comments', \n",
    "                 'human_alignment', 'asr_response', 'asr_comparison', 'audio_length', 'process_time']\n",
    "df = df[desired_order]\n",
    "\n",
    "\n",
    "### Handle rows with NaN values in human_transcription column ###\n",
    "print(df['human_transcription'].isnull().sum(), \"rows with NaN values in human_transcription column and notes associated with these rows are\", df[df['human_transcription'].isnull()]['notes'].unique(), \"\\n\")\n",
    "# Remove rows with NaN values in transcription column \n",
    "df = df.dropna(subset=['human_transcription']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "### Synchronize the DataFrame with the audio files by removing missing files and duplicates ###\n",
    "df, recordings_filepaths, recordings_filenames = match_files_df_recordings(df = df, \n",
    "                                                                            recordings_folder_path = recordings_folder_path, \n",
    "                                                                            path_filters = path_filters)\n",
    "\n",
    "\n",
    "df['reference_text'] = df['reference_text'].apply(lambda x: x.replace(\";\", \" \"))\n",
    "df['filepath'] = [os.path.join(recordings_folder_path, filename) for filename in df['filename']]\n",
    "\n",
    "# Copy accuracy column to accuracy_og column\n",
    "df['accuracy'] = df['accuracy'].apply(lambda x: x.strip())\n",
    "df['accuracy'] = df['accuracy'].apply(lambda x: \" \".join([i for i in x.split(\" \") if i != \"\"]))\n",
    "df['accuracy_og'] = df['accuracy']\n",
    "\n",
    "df.at[339, 'accuracy'] = df.at[339, 'accuracy'] + \" NA\" if len(df.at[339, 'accuracy'].split(\" \")) < 13 else df.at[339, 'accuracy']\n",
    "# for each filename in df, find the corresponding row in df_ and print if their accuracy column is not equal\n",
    "for idx, row in df.iterrows():\n",
    "    filename = row['filename']\n",
    "    accuracy = row['accuracy']\n",
    "    accuracy_ = df_[df_['file_name'] == filename]['accuracy'].values[0]\n",
    "    # if accuracy_ is NaN, set it to 12 NA zeros\n",
    "    # remove space before and after accuracy string\n",
    "    accuracy_ = accuracy_.strip()\n",
    "    if accuracy != accuracy_:\n",
    "        # show the index of mismatch in the two accuracy columns\n",
    "        index = [i for i in range(12) if accuracy.split(\" \")[i] != accuracy_.split(\" \")[i]]\n",
    "        # change the accuracy column in df to match the accuracy column in df_\n",
    "        df.at[idx, 'accuracy'] = accuracy_\n",
    "\n",
    "# Fill the audio_length column by adding the audio duration for each file in recordings_filepaths and get the total time\n",
    "total_time = 0\n",
    "for audio_path in recordings_filepaths:\n",
    "    filename = os.path.basename(audio_path)\n",
    "    if filename in df['filename'].values:\n",
    "        audio_length = librosa.get_duration(path=audio_path)\n",
    "        df.loc[df['filename'] == filename, 'audio_length'] = audio_length\n",
    "        total_time += audio_length\n",
    "print(\"\\nTotal time of audio files:\", total_time, \"seconds =\", total_time/60, \"minutes\")\n",
    "\n",
    "# replace all the 'NA' with 0 in the accuracy column\n",
    "df['accuracy'] = df['accuracy'].apply(lambda x: x.replace(\"NA\", \"0\"))\n",
    "\n",
    "# Save the updated df to a JSON file\n",
    "df.to_json('dfs/deco/Intervention_Decoding_og_df.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f3179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the number of unique reference text and config_id match\n",
    "assert len(df['reference_text'].unique()) == len(df['config_id'].unique()), \"Number of unique reference text and config_id do not match\"\n",
    "print(\"Number of unique reference text and config_id match and is\", len(df['reference_text'].unique()), \"\\n\")\n",
    "\n",
    "# Each config_id should have the same reference_text print them\n",
    "for config_id in sorted(df['config_id'].unique()):\n",
    "    print(config_id, df[df['config_id'] == config_id]['reference_text'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad42a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print if a participant has multiple session_form values\n",
    "print(\"\\nNumber of participants with multiple session_form (A, B, C) values: \", len(df.groupby('participant_id')['session_form'].nunique()[df.groupby('participant_id')['session_form'].nunique() > 1]))\n",
    "\n",
    "# Print the number of participants per language and trial\n",
    "for lan in df['language'].unique():\n",
    "    print(\"\\n Total number of unique participants for each config_id in\", lan)\n",
    "    for config_id in df['config_id'].unique():\n",
    "        filtered_df = df[(df['language'] == lan) & (df['config_id'] == config_id)]\n",
    "        # print number of participants per config_id\n",
    "        print(str(config_id) + \" :  \" + str(len(filtered_df['participant_id'].unique())))\n",
    "    print(\" \\nTotal number of unique participants for each phase in\", lan)\n",
    "    for phase in df['phase'].unique():\n",
    "        filtered_df = df[(df['language'] == lan) & (df['phase'].apply(lambda x: x.split(\"_\")[0] in [phase]))]\n",
    "        print(phase + \" :  \" + str(len(filtered_df['participant_id'].unique())) + \" participants\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10226509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first 5 rows of the dfprint reference_text, human_transcription, and accuracy columns\n",
    "for idx, row in df.iterrows():\n",
    "    print(row['reference_text']) \n",
    "    print(row['human_transcription'])\n",
    "    print(row['accuracy'])\n",
    "    print(row['config_id'])\n",
    "    if idx == 10:\n",
    "        break \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec1dc33",
   "metadata": {},
   "source": [
    "## Transcription Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c14579",
   "metadata": {},
   "source": [
    "Get transcriptions without the annotations and write them in the words_human_transcription column. These will be fed to the ASR during training so they need to contain what was said without any comment or mark.\n",
    "- Remove the sounding out annotation '.' in the transcription : pa.pa --> papa\n",
    "- Remove the insertion annotation'{...}' in the transcription : pa{pu}pa --> papupa\n",
    "- Remove the comments {inaudible}, {comments}, {adult_support}, {adult_task}, {no audio}, {pause}, {cut recording}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1ecdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the words_human_transcription column with the human transcription with removed annotations\n",
    "def remove_transcription_annotations(transcription):\n",
    "    \"\"\"\n",
    "    Removes clinician annotations from a given transcription.\n",
    "    Args:\n",
    "        transcription (str): The input transcription string.\n",
    "    Returns:\n",
    "        str: The transcription string with annotations removed.\n",
    "    \"\"\"\n",
    "    # If the letter before and after the \".\" is the same, only keep one letter and '.' ex: \"a.ar\" -> \"ar\"\n",
    "    transcription = re.sub(r'(\\w)\\.(\\1)', r'\\2', transcription)\n",
    "    transcription = transcription.replace(\".\", \"\")\n",
    "\n",
    "    transcription = transcription.replace(\"{inaudible}\", \"\")\n",
    "    transcription = transcription.replace(\"{comments}\", \"\")\n",
    "    transcription = transcription.replace(\"{adult_support}\", \"\")\n",
    "    transcription = transcription.replace(\"{adult_task}\", \"\")\n",
    "    transcription = transcription.replace(\"{no audio}\", \"\")\n",
    "    transcription = transcription.replace(\"{pause}\", \"\")\n",
    "    transcription = transcription.replace(\"{cut recording}\", \"\")\n",
    "\n",
    "    transcription = transcription.replace(\"{\", \" \")\n",
    "    transcription = transcription.replace(\"}\", \" \")\n",
    "    \n",
    "    # remove all between brackets and parenthesis\n",
    "    transcription = re.sub(r'\\[.*?\\]', '', transcription)\n",
    "    transcription = re.sub(r'\\(.*?\\)', '', transcription)\n",
    "\n",
    "    return transcription\n",
    "\n",
    "df['words_human_transcription'] = df['human_transcription'].apply(lambda x: remove_transcription_annotations(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7e1c2",
   "metadata": {},
   "source": [
    "The transcriptions in our dataset exhibit inconsistencies where the same word can be spelled differently across rows. To ensure consistent labeling for our finetuning task and accurate evaluation of the ASR performance, we need to normalize the spelling of correctly uttered words. To achieve this, we create a replacement dictionary that maps each variant spelling to a normalized spelling. \n",
    "\n",
    "To create the replacement dictionary, we iterate over each list of (pseudo)words. For each (pseudo)word in this list, we select only the rows where the clinician marked the word as correctly pronounced (where its value in the accuracy list is 2). We then check for discrepancies between the reference text and human transcription for these rows and add any identified discrepancies to our dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83c7cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for replacements\n",
    "# To find all spelling of a word I did not consider the utterances where score for the word is 0\n",
    "replacements = {\n",
    "    \"rui\": \"ruit\",\n",
    "    \"digasscelle\":\"digacelle\",\n",
    "    \"parosse\": \"paroce\",\n",
    "    \"re\": \"reux\",\n",
    "    \"reu\": \"reux\",\n",
    "    \"bante\": \"bente\",\n",
    "    \"dui\": \"duie\",\n",
    "    \"djui\": \"duie\",\n",
    "    \"englage\": \"anglage\",\n",
    "    \"anglaje\": \"anglage\",\n",
    "    \"trèb\": \"trèbe\",\n",
    "    \"piro\": \"pireau\",\n",
    "    \"lizoi\": \"lisoie\",\n",
    "    \"milvené\": \"milvenet\",\n",
    "    \"conjo\": \"conjeau\",\n",
    "    \"éteur\": \"héteur\",\n",
    "    \"eteur\": \"héteur\",\n",
    "    \"pléfassion\": \"pléfation\",\n",
    "    \"sorpil\": \"sorpile\",\n",
    "    \"é\": \"haie\",\n",
    "    \"aie\": \"haie\",\n",
    "    \"ai\": \" haie\",    \n",
    "    \"chanbre\": \"chambre\",\n",
    "    \"crane\": \"crâne\",\n",
    "    \"hotel\": \"hôtel\",\n",
    "    \"otel\": \"hôtel\",    \n",
    "    \"otèl\": \"hôtel\",\n",
    "    \"pomier\": \"pommier\",\n",
    "    \"pomié\": \"pommier\",\n",
    "    \"pommié\": \"pommier\",\n",
    "    \"cœur\": \"coeur\",\n",
    "    \"keur\": \"coeur\",\n",
    "    \"baskét\": \"basket\",\n",
    "    \"baskèt\": \"basket\",\n",
    "    \"réci\": \"récit\",\n",
    "    \"réssi\": \"récit\",\n",
    "    \"sette\": \"sept\",\n",
    "    \"séte\": \"sept\",\n",
    "    \"clé\": \"clef\",\n",
    "    \"tradission\": \"tradition\",\n",
    "    \"sisse\": \"six\",\n",
    "    \"sis\": \"six\",\n",
    "    \"cieu\": \"cieux\",\n",
    "    \"sieu\": \"cieux\",\n",
    "    \"départemen\": \"département\",\n",
    "    \"éco\": \"écho\",\n",
    "    \"éko\": \"écho\",\n",
    "    \"logie\": \"logis\",\n",
    "    \"logi\": \"logis\",\n",
    "    \"intéligence\": \"intelligence\",\n",
    "    \"intélligence\": \"intelligence\",\n",
    "    \"inteligence\": \"intelligence\",\n",
    "    \"oposition\": \"opposition\",\n",
    "    \"opozission\": \"opposition\",\n",
    "    \"creu\": \"creux\",\n",
    "    \"foir\": \"foire\",\n",
    "    \"chouétte\": \"chouette\",\n",
    "    \"chouéte\": \"chouette\",\n",
    "    \"cuizinié\": \"cuisinier\",\n",
    "    \"cuisinié\": \"cuisinier\",\n",
    "    \"cuisiniez\": \"cuisinier\",\n",
    "    \"céllier\": \"cellier\",\n",
    "    \"sélié\": \"cellier\",\n",
    "    \"célier\": \"cellier\",\n",
    "    \"éstragon\": \"estragon\",\n",
    "    \"instrument\":\"instruments\",\n",
    "    \"instrumen\":\"instruments\",\n",
    "    \"malédiksion\":\"malédiction\",\n",
    "    \"malédikssion\":\"malédiction\",\n",
    "    \"lé\": \"lait\",\n",
    "    \"lai\": \"lait\",\n",
    "    \"dézir\": \"désir\",\n",
    "    \"joi\": \"joie\",\n",
    "    \"éskalope\": \"escalope\",\n",
    "    \"eskalope\": \"escalope\",\n",
    "    \"béré\": \"béret\",\n",
    "    \"boulo\": \"boulot\",\n",
    "    \"bouleau\": \"boulot\",\n",
    "    \"bohneur\": \"bonheur\",\n",
    "    \"boneur\": \"bonheur\",\n",
    "    \"boneur\": \"bonheur\",\n",
    "    \"paté\": \"pâté\",\n",
    "    \"voi\": \"voix\",\n",
    "    \"voie\": \"voix\",\n",
    "    \"fassade\": \"façade\",\n",
    "    \"argil\": \"argile\",\n",
    "    \"gigo\": \"gigot\",\n",
    "    \"jigo\": \"gigot\",\n",
    "    \"duvé\": \"duvet\",\n",
    "    \"convalésense\": \"convalescence\",\n",
    "    \"convalésensse\": \"convalescence\",\n",
    "    \"exitation\": \"excitation\",\n",
    "    \"exsitation\": \"excitation\",\n",
    "    \"exsitassion\": \"excitation\",\n",
    "    \"éksitassion\": \"excitation\",\n",
    "    \"eksitassion\": \"excitation\",\n",
    "    \"tron\": \"tronc\",\n",
    "    \"poir\": \"poire\",\n",
    "    \"mouettte\": \"mouette\",\n",
    "    \"menuizier\": \"menuisier\",\n",
    "    \"menuizié\": \"menuisier\",\n",
    "    \"casié\": \"casier\",\n",
    "    \"cazié\": \"casier\",\n",
    "    \"cazier\": \"casier\",\n",
    "    \"équateur\": \"equateur\",\n",
    "    \"orkestre\": \"orchestre\",\n",
    "    #\"orchkestre\": \"orchestre\",\n",
    "    \"orkéstre\": \"orchestre\",\n",
    "    \"réklamation\": \"réclamation\",\n",
    "    \"réklamassion\": \"réclamation\",\n",
    "    \"réclamassion\": \"réclamation\",\n",
    "    \"rai\": \"raie\",\n",
    "    \"ré\": \"raie\",\n",
    "    \"rré\": \"raie\",\n",
    "    \"grève\": \"grève\",\n",
    "    \"stan\": \"stand\",\n",
    "    \"stande\": \"stand\",\n",
    "    \"crète\": \"crête\",\n",
    "    \"créte\": \"crête\",\n",
    "    \"saizon\": \"saison\",\n",
    "    \"sézon\": \"saison\",\n",
    "    \"pompié\": \"pompier\",\n",
    "    \"ponpié\": \"pompier\",\n",
    "    \"sœur\": \"soeur\",\n",
    "    \"ser\": \"soeur\",\n",
    "    \"seur\": \"soeur\",\n",
    "    \"sseur\": \"soeur\",\n",
    "    \"regré\": \"regret\",\n",
    "    \"rregré\": \"regret\",\n",
    "    \"fusi\": \"fusil\",\n",
    "    \"fuzi\": \"fusil\",\n",
    "    \"fisse\": \"fils\",\n",
    "    \"aout\": \"août\",\n",
    "    \"out\": \"août\",\n",
    "    \"oute\": \"août\",\n",
    "    \"producsion\": \"production\",\n",
    "    \"produkssion\": \"production\",\n",
    "    \"dieu\": \"dieux\",\n",
    "    \"tenssion\": \"tension\",\n",
    "    \"déménagemen\": \"déménagement\",\n",
    "    \"toi\": \"toit\",\n",
    "    \"join\": \"juin\",\n",
    "    \"éskapade\": \"escapade\",\n",
    "    \"eskapade\": \"escapade\",\n",
    "    \"navé\": \"navet\",\n",
    "    \"pante\": \"pente\",\n",
    "    \"bandi\": \"bandit\",\n",
    "    \"choi\": \"choix\",\n",
    "    \"parade\": \"parade\",\n",
    "    \"fossil\": \"fossile\",\n",
    "    \"tui\": \"tuit\",\n",
    "    \"cutisse\": \"cutice\",\n",
    "    \"ze\": \"zeux\",\n",
    "    \"bummon\": \"bumon\",\n",
    "    \n",
    "    \"pérné\": \"perné\",\n",
    "    \"pernet\": \"perné\",\n",
    "    \"bonfaje\": \"bonfage\",\n",
    "    \"jel\": \"jeul\",\n",
    "    \"tréme\": \"trème\",\n",
    "    \"bukurelle\": \"bucurelle\",\n",
    "    \"davo\": \"daveau\",\n",
    "    \"ninoi\": \"ninoie\",\n",
    "    \"trefoné\": \"trefonet\",\n",
    "    \"tréfonet\": \"trefonet\",\n",
    "    \"tréfoné\": \"trefonet\",\n",
    "    \"tièj\": \"tiège\",\n",
    "    \"tiééj\": \"tiège\",\n",
    "    \"ronjo\": \"ronjeau\",\n",
    "    \"trufondulance\": \"trufondulence\",\n",
    "    \"uper\": \"hupeur\",\n",
    "    \"gralassion\": \"gralation\",\n",
    "    \"turlem\": \"turlème\",\n",
    "    \"escro\": \"escroc\",\n",
    "    \"eskro\": \"escroc\",\n",
    "    \"éskro\": \"escroc\",\n",
    "    \"éscro\": \"escroc\",\n",
    "    \"taba\": \"tabac\",\n",
    "    \"reconaissance\": \"reconnaissance\",\n",
    "    \"rekonnaissance\": \"reconnaissance\",\n",
    "    \"reconéssance\": \"reconnaissance\",\n",
    "    \"reconnéssance\": \"reconnaissance\",\n",
    "    \"aparition\": \"apparition\",\n",
    "    \"apparition\": \"apparition\",\n",
    "    \"aparission\": \"apparition\",\n",
    "    \"apparission\": \"apparition\",\n",
    "    \"blan\": \"blanc\",\n",
    "    \"poi\": \"poids\",\n",
    "    \"couète\": \"couette\",\n",
    "    \"chemizié\": \"chemisier\",\n",
    "    \"chemizier\": \"chemisier\",\n",
    "    \"rozier\": \"rosier\",\n",
    "    \"rozié\": \"rosier\",\n",
    "    \"ékuyière\": \"écuyère\",\n",
    "    \"ékuyère\": \"écuyère\",\n",
    "    \"ékuiyère\": \"écuyère\",\n",
    "    \"catastrof\": \"catastrophe\",\n",
    "    \"catastrofe\": \"catastrophe\",\n",
    "    \"déklarassion\": \"déclaration\",\n",
    "    \"déclarassion\": \"déclaration\",\n",
    "    \"pièje\": \"piège\",\n",
    "    \"pièje\": \"piège\",\n",
    "    \"fame\": \"femme\",\n",
    "    \"ffamme\": \"femme\",\n",
    "    \"ffame\": \"femme\",\n",
    "    \"secré\": \"secret\",\n",
    "    \"ssecré\": \"secret\",\n",
    "    \"fini\": \"finit\",\n",
    "    \"mile\": \"mille\",\n",
    "    \"sère\": \"cerf\",\n",
    "    \"cer\": \"cerf\",\n",
    "    \"cèr\": \"cerf\",\n",
    "    \"cér\": \"cerf\",\n",
    "    \"présision\": \"précision\",\n",
    "    \"présizion\": \"précision\",\n",
    "    \"préssizion\": \"précision\",\n",
    "    \"précizion\": \"précision\",\n",
    "    \"disse\": \"dix\",\n",
    "    \"lieu\": \"lieux\",\n",
    "    \"milion\": \"million\",\n",
    "    \"débarkement\": \"débarquement\",\n",
    "    \"débarkemen\": \"débarquement\",\n",
    "    \"débarquemen\": \"débarquement\",\n",
    "    \"nui\": \"nuit\",\n",
    "    \"jou\": \"joue\",\n",
    "    \"éscalade\": \"escalade\",\n",
    "    \"éskalade\": \"escalade\",\n",
    "    \"eskalade\": \"escalade\",\n",
    "    \"valé\": \"valet\",\n",
    "    \"jalou\": \"jaloux\",\n",
    "    \"noi\": \"noix\",\n",
    "    \"réptile\": \"reptile\",\n",
    "    \"réptil\": \"reptile\",\n",
    "    \"bé\": \"baie\",\n",
    "    \"bai\": \"baie\",\n",
    "    \"frére\": \"frère\",\n",
    "    \"nombr\": \"nombre\",\n",
    "    \"comande\": \"commande\",\n",
    "    \"crèpe\": \"crêpe\",\n",
    "    \"crèpes\": \"crêpe\",\n",
    "    \"pinsson\": \"pinson\",\n",
    "    \"jui\": \"juit\",\n",
    "    \"goi\": \"goix\",\n",
    "    \"gua\": \"goix\",\n",
    "    \"ronbage\": \"rombage\",\n",
    "    \"ronbaje\": \"rombage\",\n",
    "    \"rombaje\": \"rombage\",\n",
    "    \"jer\": \"jeur\",\n",
    "    \"sidomèle\": \"sidomelle\",\n",
    "    \"tivo\": \"tiveau\",\n",
    "    \"firoi\": \"firoie\",\n",
    "    \"timpuner\": \"timpunet\",\n",
    "    \"timpuné\": \"timpunet\",\n",
    "    \"tinpuné\": \"timpunet\",\n",
    "    \"tinpunet\": \"timpunet\",\n",
    "    \"fème\": \"phème\",\n",
    "    \"tanjo\": \"tanjeau\",\n",
    "    \"rœil\": \"reuil\",\n",
    "    \"opeur\": \"hopeur\",\n",
    "    \"oper\": \"hopeur\",\n",
    "    \"nive\": \"nive\",\n",
    "    \"jorcassion\": \"jorcation\",\n",
    "    \"jorkassion\": \"jorcation\",\n",
    "    \"jorcation\": \"jorcation\",\n",
    "    \"jorkation\": \"jorcation\",\n",
    "    'jof': 'jauf',\n",
    "    'reptil': 'reptile',\n",
    "    'rekonéssance': 'reconnaissance',\n",
    "    'reconnaissance': 'reconnaissance',\n",
    "    'upeur': 'hupeur',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7303b3ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884c19b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all config_ids ( = lists of (pseudo)words ) in the df\n",
    "config_ids = df['config_id'].unique()\n",
    "print(\"Number of unique config_id: \", len(config_ids))\n",
    "\n",
    "i = 0 # index of the config_id to be processed \n",
    "j = 0 # index of the word in the config_id to be processed\n",
    "\n",
    "# Get all rows with the config_id we want to process\n",
    "df_config = df[df['config_id'] == config_ids[i]]\n",
    "\n",
    "print(\"Processed config_id: \", config_ids[i])\n",
    "print(\"Processed reference_text:\", df_config.iloc[0]['reference_text'])\n",
    "print(\"Processed word in reference_text:\", df_config.iloc[0]['reference_text'].split(\" \")[j])\n",
    "\n",
    "\n",
    "### Get all rows where the j element of the accuracy list is 2 (if the accuracy list is bigger than j) ###\n",
    "# Convert accuracy column to a list of integers\n",
    "df_config.loc[:, 'accuracy'] = df_config['accuracy'].apply(lambda x: [int(i) for i in x.split(\" \") if i.isdigit()])\n",
    "\n",
    "df_config = df_config[df_config['accuracy'].apply(lambda x: len(x) > j)]\n",
    "df_config = df_config[df_config['accuracy'].apply(lambda x: x[j] != 0)]\n",
    "\n",
    "# Show all rows where the j word in the reference_text column is not contained in the string in the words_human_transcription column\n",
    "df_config = df_config[df_config.apply(lambda x: x['reference_text'].split(\" \")[j] not in x['words_human_transcription'], axis=1)]\n",
    "df_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635c5cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a space before and after each key and value in the replacements dictionary to avoid replacing parts of words\n",
    "# For example \"re\" in \"arbre\" will not replaced by \"reux\"\n",
    "replacements = {f\" {k} \": f\" {v} \" for k, v in replacements.items()}\n",
    "# Add a space before each sentence in words_human_transcription\n",
    "df['words_human_transcription'] = df['words_human_transcription'].apply(lambda x: f\" {x} \")\n",
    "df_not_normalized = df.copy()\n",
    "\n",
    "# Replace the keys with the values in the replacements dictionary\n",
    "for k, v in replacements.items():\n",
    "    df['words_human_transcription'] = df['words_human_transcription'].str.replace(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef8fc60",
   "metadata": {},
   "source": [
    "Code to check all the transcriptions containing a given word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a01190",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'ze'\n",
    "df_row = df_not_normalized.loc[df_not_normalized['words_human_transcription'].str.contains(' '+word+' ')]\n",
    "df_row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f3202b",
   "metadata": {},
   "source": [
    "After the normalization of the spelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b691ec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'ze'\n",
    "df_row = df.loc[df['words_human_transcription'].str.contains(' '+word+' ')]\n",
    "df_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b237de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889e9369",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_accuracy_change = [\n",
    "    (\"3185_edugame2023_eb2dc813ac324ef5b2d08b499c9b0195_e3c0dd9c87e04709b975d6dfd85edbf2.wav\", [2, 1, 2, 2, 2, 1, 0, 1, 0, 0, 0, 1]),\n",
    "    (\"4006_edugame2023_97a6892868d5420687fa49368f83b649_88b0ebe874f44e12b10cf6332efc8e20.wav\", [2, 1, 1, 1, 2, 2, 2, 1, 2, 1, 0, 0]),\n",
    "    (\"3122_edugame2023_b9898abefb3445ada6d0490a7251f340_b75faaac524446539bb181d8a5d24d73.wav\", [0, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1]),\n",
    "    (\"3722_edugame2023_6aa06c9452704d77aac910356caaf078_ab8320445b3841b083e01a53b09a547d.wav\", [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]),\n",
    "    (\"3726_edugame2023_04781866635f47c38e712760a4206223_e09682d5d140493ab8c00158e4e49a66.wav\", [0, 1, 0, 0, 2, 1, 0, 1, 1, 2, 1, 1]),\n",
    "    (\"3722_edugame2023_6aa06c9452704d77aac910356caaf078_ab8320445b3841b083e01a53b09a547d.wav\", [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]),\n",
    "    (\"3184_edugame2023_18b68dd83a4b4506a5a8db11cf640e67_11e234fc5b5a45b7ba26ffc041fe04d4.wav\", [0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0]),\n",
    "    (\"3722_edugame2023_6aa06c9452704d77aac910356caaf078_ab8320445b3841b083e01a53b09a547d.wav\", [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
    "    (\"3111_edugame2023_9c0eb20452134660afa8aa9b8cc58579_ac6125a3f26649d38e25ac1f8c11d96d.wav\", [1, 2, 0, 1, 2, 1, 2, 0, 2, 2, 1, 1]),\n",
    "    (\"3147_edugame2023_650080f2882548949fbcdac62d326398_b218dd10c2c44fa9b5a7b3d1af2c86c0.wav\", [2, 2, 2, 1, 0, 2, 1, 1, 1, 2, 1, 2]),\n",
    "    (\"4012_edugame2023_c5f994b9a174432b826565f57098ca1a_bee3d3f7284e4b5eb7cc51a9611e9705.wav\", [2, 2, 1, 0, 2, 0, 1, 0, 2, 2, 2, 2]),\n",
    "    (\"3125_edugame2023_8fe9370b1fff4db4bd4dc98613c4ecff_77cc7df1af174993b413df44cb3e5e4c.wav\", [1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1]),\n",
    "    (\"3738_edugame2023_2b7f897c8b74431697be15e3ccf65a4f_cc163642455c4ae6b02d1d25f8cd7db0.wav\", [0, 2, 1, 0, 2, 0, 1, 0, 2, 2, 1, 0]),\n",
    "    (\"3184_edugame2023_31e9ea5172f84cdfa0615027d60d61ba_ca59c03c6cf64cffafe9d5fa0c76ebbd.wav\", [0, 0, 2, 0, 1, 2, 1, 0, 1, 2, 1, 1]),\n",
    "    (\"3158_edugame2023_a6fc181fb45d49fcb199afc489ee08bd_adbe989158fc4a419176d723c1cfa539.wav\", [2, 1, 2, 1, 1, 1, 0, 0, 1, 2, 0, 0]),\n",
    "    (\"3142_edugame2023_b5e0a9ac5743470eb758689a59610bab_5b17b70717ba45a091298c7896f2b036.wav\", [1, 1, 0, 0, 2, 2, 0, 0, 0, 0, 0, 1]),\n",
    "    (\"3108_edugame2023_f4922707ff73491a85766bbcd62ef92f_5c27530df514442794960fd7201e83f1.wav\", [0, 1, 0, 1, 0, 2, 1, 0, 0, 0, 1, 1]),\n",
    "    (\"3169_edugame2023_993e50a9ebbd471a8af41f6869e05a8a_fd8af6717dba4e219886a4d30a0f7cd0.wav\", [1, 1, 0, 0, 0, 2, 2, 1, 1, 0, 0, 0]),\n",
    "    (\"3109_edugame2023_36a8df2696404c11985b72bc9a41bd72_f473e404e593433a9ddc524334856fe3.wav\", [2, 1, 1, 0, 2, 1, 1, 0, 1, 1, 2, 0]),\n",
    "    (\"3169_edugame2023_993e50a9ebbd471a8af41f6869e05a8a_6680c0eb270c4b49a3889b3c37043dbc.wav\", [2, 2, 2, 0, 0, 2, 0, 0, 1, 1, 1, 1]),\n",
    "    (\"3180_edugame2023_954e1af83a994241a3bfe5d1954f7b6c_d33d0fbb5f124451b6c67da40ba46edc.wav\", [2, 2, 2, 1, 2, 2, 2, 0, 2, 2, 1, 2]),\n",
    "    (\"3138_edugame2023_75285312c1be4e859b964aa0f6164829_5f5febf8ea034755b15a46e453252548.wav\", [0, 2, 0, 0, 0, 0, 2, 0, 2, 0, 1, 1]),\n",
    "    (\"3407_edugame2023_8f88e8025a4743299b7785ae1ad9922e_0d77d0c00ebb429cb867faf1f50b0874.wav\", [1, 2, 1, 0, 0, 2, 1, 0, 2, 2, 0, 1]),\n",
    "    (\"3104_edugame2023_6070e62649ff4888b5f4758f506e22f7_7979e246a17847629355c5256acc13d5.wav\", [2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 1, 0]),\n",
    "    (\"3715_edugame2023_133ad28ca5ca40bfbc0fd075d23c0b56_aab9b04fb5cb41c486aa68e2c72960d6.wav\", [2, 1, 2, 1, 0, 0, 0, 2, 2, 2, 1, 0]),\n",
    "    (\"3727_edugame2023_302578ea2c9b4c9b90c2d464cc282439_07d80a22c87343b1adfa0f161c6e7f63.wav\", [2, 1, 2, 1, 2, 0, 0, 1, 2, 2, 2, 2]),\n",
    "    (\"4023_edugame2023_2c0c9d7e7af24547ae0e809494dfc1f7_5e23ce98d6a745d7b76f354cd9b9204c.wav\", [2, 0, 0, 0, 2, 0, 0, 2, 2, 2, 1, 0]),\n",
    "    (\"3151_edugame2023_60c1231162604cd9ad81ca6c717ca38e_1abd9770763e4a5e80dda765d59413b0.wav\", [0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0]),\n",
    "    (\"3724_edugame2023_feab4682e57641e6a52094551121bdc2_f1c75f122e9243059e0c08be59982a9a.wav\", [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
    "    (\"3151_edugame2023_60c1231162604cd9ad81ca6c717ca38e_1abd9770763e4a5e80dda765d59413b0.wav\", [0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0]),\n",
    "    (\"3102_edugame2023_b138ec8efbd74ef4a5ae3c3c941907ea_cf80243db2fa41cd954544e152c69232.wav\", [0, 2, 2, 2, 0, 2, 0, 0, 0, 0, 0, 0]),\n",
    "    (\"3143_edugame2023_c7acb8b9615147deb6f1032edf65aea6_04693034c1ce4d65bf2103428f60a3c4.wav\", [0, 2, 1, 1, 2, 0, 0, 1, 1, 0, 0, 1]),\n",
    "    (\"3723_edugame2023_abbc25c5f4d34f8aad4c55484d7193b0_01e7c48162154f12978a4997163ec327.wav\", [0, 0, 2, 1, 2, 2, 2, 1, 2, 0, 0, 2]),\n",
    "    (\"4007_edugame2023_bf4e28a282db4656b2b1c8475c2929a9_ee8710f9ad264ec0848dfd1922fe8c58.wav\", [0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2]),\n",
    "    (\"3723_edugame2023_abbc25c5f4d34f8aad4c55484d7193b0_1c252027d4c348c2ac832836d4ebea73.wav\", [2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 0]),\n",
    "    (\"3168_edugame2023_63642ca494544fac819152eb8b0935de_44af16457b494216a50322517868b68c.wav\", [2, 1, 2, 0, 2, 1, 0, 1, 0, 1, 2, 1]),\n",
    "    (\"3424_edugame2023_a3174ee9d0ff45a9b9bbf07f9a7abef1_827c2cab29f7450dab42f28ca7d5d084.wav\", [2, 1, 2, 0, 2, 1, 0, 2, 2, 1, 1, 2]),\n",
    "    (\"3411_edugame2023_dc2803354f504d52b183f115e06fe140_8bc4a0ee5a20402b9c00010e4f64ddb2.wav\", [2, 1, 1, 1, 2, 1, 0, 0, 2, 1, 2, 1]),\n",
    "    (\"3753_edugame2023_80ef3443d7bd49b597ef9159c5c712bd_c7aae3c47a0b45bb81ac3c3e4ee9ede2.wav\", [1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]),\n",
    "    (\"3723_edugame2023_299b733d8cda4565beb8895b9000f3b7_d036def5fd4f4fca97f7e9961f7a4d24.wav\", [1, 1, 1, 0, 2, 1, 0, 1, 1, 2, 0, 2]),\n",
    "    (\"4013_edugame2023_1620c0ec6c0d482a96954e681fd5260f_1c3d9ba9c45a4daea91ca50cd9ac2ec9.wav\", [1, 2, 1, 1, 0, 1, 0, 0, 2, 2, 0, 1]),\n",
    "    (\"3155_edugame2023_f3467c3935ed43be849ec26ac7aaab86_8678cc14130248ab81708e7d5722f336.wav\", [1, 0, 1, 1, 1, 0, 2, 1, 2, 2, 1, 0]),\n",
    "    (\"3722_edugame2023_6aa06c9452704d77aac910356caaf078_ab8320445b3841b083e01a53b09a547d.wav\", [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
    "    (\"3722_edugame2023_6aa06c9452704d77aac910356caaf078_ab8320445b3841b083e01a53b09a547d.wav\", [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
    "    (\"3151_edugame2023_60c1231162604cd9ad81ca6c717ca38e_1abd9770763e4a5e80dda765d59413b0.wav\", [0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0]),\n",
    "    (\"3156_edugame2023_c183500d36fd40869ec0e3971031afcc_0fe79f18b8104f1e8761cdc4b2f398bf.wav\", [2, 1, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0]),\n",
    "    (\"3710_edugame2023_9d66bb18ffc54188a0a8eb00785d93db_6bb0d5e00b9c43a7bb35107f65da5e60.wav\", [2, 2, 0, 1, 2, 1, 0, 1, 2, 1, 2, 2]),\n",
    "    (\"3185_edugame2023_eb2dc813ac324ef5b2d08b499c9b0195_ab74571a9a0a4223a4436224c53f1c75.wav\", [0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0]),\n",
    "    (\"3107_edugame2023_66a9dee045c844c0a31d803c4992414c_7f3a77ac7cc04195b0970f9114532ba8.wav\", [2, 2, 0, 0, 0, 2, 0, 1, 1, 1, 0, 1]),\n",
    "    (\"3107_edugame2023_66a9dee045c844c0a31d803c4992414c_3453b2a9513d41fdba7a85293ce23bc8.wav\", [0, 0, 2, 1, 2, 2, 0, 0, 0, 0, 1, 1]),\n",
    "    (\"3184_edugame2023_31e9ea5172f84cdfa0615027d60d61ba_ca59c03c6cf64cffafe9d5fa0c76ebbd.wav\", [0, 0, 2, 0, 1, 2, 1, 1, 1, 2, 1, 1]),\n",
    "    (\"3161_edugame2023_9889b3877b724fb0abdce7aa9c0daffc_b7b7ef370786475eb73c1d56af3358d1.wav\", [0, 1, 0, 1, 2, 2, 1, 0, 0, 0, 0, 0]),\n",
    "    (\"3108_edugame2023_f4922707ff73491a85766bbcd62ef92f_825cd09d574f4fdf9ee839688959b02b.wav\", [1, 2, 1, 1, 0, 1, 2, 1, 2, 2, 1, 1]),\n",
    "    (\"3119_edugame2023_961f25cb239c493f8dc868646059fe60_bd422dbdc0554417985de26a5770fdbd.wav\", [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]),\n",
    "    (\"3403_edugame2023_ce4ce6660a524aa1b20d24092632d43b_d216811b566d4213b002c5b24c7d4eb6.wav\", [1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0]),\n",
    "    (\"3150_edugame2023_8c2562ac45b543e3814d7230261ccaac_85ea381846c34845b2406fa419b4222d.wav\", [2, 1, 0, 2, 1, 0, 0, 2, 1, 0, 1, 1]),\n",
    "    (\"3177_edugame2023_4e6107c4aac043c592aa870d006b95e9_fcbd089589fd4e548f68f3cedd677ed7.wav\", [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1]),\n",
    "    (\"3177_edugame2023_4e6107c4aac043c592aa870d006b95e9_fcbd089589fd4e548f68f3cedd677ed7.wav\", [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1]),\n",
    "    (\"3749_edugame2023_f4704c3cddcf4bc6b6ba6c73fba064ab_368a44dd0b2d4ac396ff8883148b11ea.wav\", [1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
    "    \n",
    "    ]\n",
    "\n",
    "eu_list = ['dieu', 'creu', 'lieu', 'cieu', 'reu', 'jeu', 'zeu']\n",
    "\n",
    "tuple_insertion = []\n",
    "tuple_soundingout = []\n",
    "tuple_eu = []\n",
    "\n",
    "insertion_changes = []\n",
    "sounding_out_changes = []\n",
    "eu_changes = [('4011_edugame2023_d7b4c8c9103f4b15852a4d530293a2af_c26218cebeec4d1ba464a7bb9d225dae.wav', 4)]\n",
    "\n",
    "insertion_count = 0\n",
    "sounding_out_count = 0\n",
    "eu_count = 0\n",
    "\n",
    "for filename, i in insertion_changes:\n",
    "    index = df[df['filename'] == filename].index[0]\n",
    "    new_acc = df.loc[index, 'accuracy'].split(\" \")\n",
    "    if new_acc[i] != '0':\n",
    "        new_acc[i] = '0'\n",
    "        insertion_count += 1\n",
    "    df.loc[index, 'accuracy'] = \" \".join(new_acc)\n",
    "\n",
    "for filename, i in sounding_out_changes:\n",
    "    index = df[df['filename'] == filename].index[0]\n",
    "    new_acc = df.loc[index, 'accuracy'].split(\" \")\n",
    "    if new_acc[i] == '0':\n",
    "        new_acc[i] != '0'\n",
    "        sounding_out_count += 1\n",
    "    df.loc[index, 'accuracy'] = \" \".join(new_acc)\n",
    "\n",
    "for filename, i in eu_changes:\n",
    "    index = df[df['filename'] == filename].index[0]\n",
    "    new_acc = df.loc[index, 'accuracy'].split(\" \")\n",
    "    if new_acc[i] != '0':\n",
    "        new_acc[i] == '0'\n",
    "        eu_count += 1\n",
    "    df.loc[index, 'accuracy'] = \" \".join(new_acc)\n",
    "\n",
    "# for each file in filepath_accuracy_change find the corresponding row in the df and print the filename, the human transcription and the og_accuracy and accuracy after the change\n",
    "for filepath, accuracy in filepath_accuracy_change:\n",
    "    # transform the accuracy list to a string\n",
    "    accuracy = \" \".join([str(i) for i in accuracy])\n",
    "    og_accuracy = df[df['filename'] == filepath]['accuracy_og'].values[0]\n",
    "    # replace all NA by 0 and remove all spaces before and after the string\n",
    "    # replace all 'NA' by '0' in accuracy string\n",
    "    og_accuracy = og_accuracy.replace(\"NA\", \"0\")\n",
    "    og_accuracy = og_accuracy.strip()\n",
    "    #  ensure length of accuracy is 12\n",
    "    assert len(og_accuracy.split(\" \")) == 12\n",
    "    if accuracy != og_accuracy:\n",
    "    # show the index of mismatch in the two accuracy columns\n",
    "        i_insertion = [ i for i in range(12) if (accuracy.split(\" \")[i] == '0' and og_accuracy.split(\" \")[i] != '0')]\n",
    "        i_soundingout_false = [ i for i in range(12) if (accuracy.split(\" \")[i] != '0' and og_accuracy.split(\" \")[i] == '0')]\n",
    "        if i_insertion:\n",
    "            index = df[df['filename'] == filepath].index[0]\n",
    "            # replace the accuracy index in the df with 0 and all indices in i_insertion\n",
    "            new_acc = df.loc[index, 'accuracy'].split(\" \")\n",
    "            for i in i_insertion:\n",
    "                if new_acc[i] != '0':\n",
    "                    new_acc[i] = '0'\n",
    "                    insertion_count += 1\n",
    "                    tuple_insertion.append((filepath, i))\n",
    "            df.loc[index, 'accuracy'] = \" \".join(new_acc)\n",
    "        \n",
    "        if i_soundingout_false:\n",
    "            index = df[df['filename'] == filepath].index[0]\n",
    "            # replace the accuracy index in the df with 1 and all indices in i_soundingout_false\n",
    "            new_acc = df.loc[index, 'accuracy'].split(\" \")\n",
    "            for i in i_soundingout_false:\n",
    "                if new_acc[i] == '0':\n",
    "                    new_acc[i] != '0'\n",
    "                    sounding_out_count += 1\n",
    "                    tuple_soundingout.append((filepath, i))\n",
    "            df.loc[index, 'accuracy'] = \" \".join(new_acc)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "        # replace all . by nothing\n",
    "        a = row['words_human_transcription']\n",
    "        b = align_texts(a, row['reference_text'])\n",
    "        # for each word in reference text.split(\" \") check if its index in accuracy is not 0 and its aligment with the human transcription is not correct\n",
    "        for i in range(len(row['reference_text'].split(\" \"))):\n",
    "            # if its value in b is not equal to the word in reference text\n",
    "            # find tuple in b whose value with index 1 is equal to the word in reference text\n",
    "            for tuple in b:\n",
    "                if tuple[1] == row['reference_text'].split(\" \")[i]:\n",
    "                    j = b.index(tuple)\n",
    "                    if b[j][0] is not None and b[j][0].endswith('eu') and b[j][0] not in eu_list and row['accuracy'].split(\" \")[i] != '0':\n",
    "                        if row['filename'] not in ['3729_edugame2023_ee652fd5e53141f8a6e73c02097dab53_0d4cd554e29e4ce9b036b3950844f3e9.wav', '4019_edugame2023_2277afada0e2440abe5fc2d84f7becd2_5b361cfb24f843afae6bfbf448d43e75.wav', '4011_edugame2023_d7b4c8c9103f4b15852a4d530293a2af_c26218cebeec4d1ba464a7bb9d225dae.wav']:\n",
    "                            new_acc = row['accuracy'].split(\" \")\n",
    "                            new_acc[i] = '0'\n",
    "                            # back to string\n",
    "                            new_acc = \" \".join(new_acc)\n",
    "                            print( row['accuracy'], new_acc)\n",
    "                            df.loc[index, 'accuracy'] = new_acc\n",
    "                            tuple_eu.append((row['filename'], i))\n",
    "                            eu_count += 1\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Insertion count: {insertion_count}\")\n",
    "print(\"This means that the accuracy was changed from 1 to 0 for the following filename and index:\")\n",
    "print(tuple_insertion)\n",
    "print() \n",
    "print(f\"Sounding out count: {sounding_out_count}\")\n",
    "print(\"This means that the accuracy was changed from 0 to 1 for the following filename and index:\")\n",
    "print(tuple_soundingout)\n",
    "print()\n",
    "print(f\"eu count: {eu_count}\")\n",
    "print(\"This means that the accuracy was changed from 1 to 0 for the following filename and index:\")\n",
    "print(tuple_eu)\n",
    "print()\n",
    "\n",
    "# calculate the values of ratios\n",
    "total_nb_scores = 930 * 12 # 930 files with 12 words each\n",
    "insertion_ratio = insertion_count / total_nb_scores\n",
    "sounding_out_ratio = sounding_out_count / total_nb_scores\n",
    "eu_ratio = eu_count / total_nb_scores\n",
    "\n",
    "print(f\"Insertion ratio: {insertion_ratio}\")\n",
    "print(f\"Sounding out ratio: {sounding_out_ratio}\")\n",
    "print(f\"eu ratio: {eu_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab5aa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all rows where the accuracy is not length 12\n",
    "df[df['accuracy'].apply(lambda x: len(x.split(\" \")) == 12)]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00722e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all config_ids ( = lists of (pseudo)words ) in the df\n",
    "config_ids = df['config_id'].unique()\n",
    "count = 0\n",
    "i = 0 # index of the config_id to be processed \n",
    "j = 0 # index of the word in the config_id to be processed\n",
    "for i in range(len(config_ids)):\n",
    "    for j in range(12):\n",
    "        # Get all rows with the config_id we want to process\n",
    "        df_config = df[df['config_id'] == config_ids[i]]\n",
    "\n",
    "        ### Get all rows where the j element of the accuracy list is 2 (if the accuracy list is bigger than j) ###\n",
    "        df_config.loc[:, 'accuracy'] = df_config['accuracy'].apply(lambda x: [int(i) for i in x.split(\" \") if i.isdigit()])\n",
    "        df_config = df_config[df_config['accuracy'].apply(lambda x: len(x) == 12)]\n",
    "        df_config = df_config[df_config['accuracy'].apply(lambda x: x[j] != 0)]\n",
    "\n",
    "        # Show all rows where the j word in the reference_text column is not contained in the string in the words_human_transcription column\n",
    "        df_config = df_config[df_config.apply(lambda x: x['reference_text'].split(\" \")[j] not in x['words_human_transcription'], axis=1)]\n",
    "        if len(df_config) > 0:\n",
    "            #print(\"Processed config_id: \", config_ids[i])\n",
    "            #print(\"Processed reference_text:\", df_config.iloc[0]['reference_text'])\n",
    "            print(\"################# Reference text: \", df_config.iloc[0]['reference_text'])\n",
    "            print(\"######## Processed word \", df_config.iloc[0]['reference_text'].split(\" \")[j])\n",
    "            for index, row in df_config.iterrows():\n",
    "                print('filename: ', row['filename'])\n",
    "                print('normalized transcription: ', row['words_human_transcription'])\n",
    "                print('transcription: ',row['human_transcription'])\n",
    "                print('accuracy: ',row['accuracy'])\n",
    "                #print(index)\n",
    "                count += 1\n",
    "                print()\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8328908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all config_ids ( = lists of (pseudo)words ) in the df\n",
    "config_ids = df['config_id'].unique()\n",
    "count = 0\n",
    "i = 0 # index of the config_id to be processed \n",
    "j = 0 # index of the word in the config_id to be processed\n",
    "for i in range(len(config_ids)):\n",
    "    for j in range(12):\n",
    "        # Get all rows with the config_id we want to process\n",
    "        df_config = df[df['config_id'] == config_ids[i]]\n",
    "\n",
    "        ### Get all rows where the j element of the accuracy list is 2 (if the accuracy list is bigger than j) ###\n",
    "        df_config.loc[:, 'accuracy'] = df_config['accuracy'].apply(lambda x: [int(i) for i in x.split(\" \") if i.isdigit()])\n",
    "        df_config = df_config[df_config['accuracy'].apply(lambda x: len(x) == 12)]\n",
    "        df_config = df_config[df_config['accuracy'].apply(lambda x: x[j] == 0)]\n",
    "\n",
    "        # Show all rows where the j word in the reference_text column is not contained in the string in the words_human_transcription column\n",
    "        df_config = df_config[df_config.apply(lambda x: x['reference_text'].split(\" \")[j] in x['words_human_transcription'].split(\" \"), axis=1)]\n",
    "        if len(df_config) > 0:\n",
    "            #print(\"Processed config_id: \", config_ids[i])\n",
    "            #print(\"Processed reference_text:\", df_config.iloc[0]['reference_text'])\n",
    "            print(\"################# Reference text: \", df_config.iloc[0]['reference_text'])\n",
    "            processed_word = df_config.iloc[0]['reference_text'].split(\" \")[j]\n",
    "            if processed_word not in [\"reux\",]:\n",
    "                print(\"######## Processed word \", processed_word)\n",
    "                for index, row in df_config.iterrows():\n",
    "                    print('filename: ', row['filename'])\n",
    "                    print('normalized transcription: ', row['words_human_transcription'])\n",
    "                    print('transcription: ',row['human_transcription'])\n",
    "                    print('accuracy: ',row['accuracy'])\n",
    "                    #print(index)\n",
    "                    count += 1\n",
    "                    print()\n",
    "                    if index in []:#[435, 177]:\n",
    "                        # change position j of digit in accuracy to 0\n",
    "                        accuracy = row['accuracy']\n",
    "                        if accuracy[j] != 0:\n",
    "                            accuracy[j] = 0\n",
    "                            df.loc[index, 'accuracy'] = \" \".join([str(i) for i in accuracy])\n",
    "                            #display(ipd.Audio(data= row['filepath'], autoplay=True, rate=16000))\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0609ce34",
   "metadata": {},
   "source": [
    "Code to check the mismatch for a given row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7f7089",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 796\n",
    "print('Filename : ' + df.loc[index]['filename'])\n",
    "print('Reference Text : ' + df.loc[index]['reference_text'])\n",
    "print('Human Transcription : ' +  df.loc[index]['human_transcription'])\n",
    "print('Words Human Transcription : ' + df.loc[index]['words_human_transcription'])\n",
    "print('Accuracy : ' + df.loc[index]['accuracy'])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b6b2bf",
   "metadata": {},
   "source": [
    "### What are the duplicate trials ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149b59c5",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc70d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all accuracy values are 12 length\n",
    "df[df['accuracy'].apply(lambda x: len(x.split(\" \")) != 12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a87802",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_not_cleaned = df.copy()\n",
    "print(\"Total number of rows in df  before removal: \", len(df))\n",
    "\n",
    "df = df[~df['notes'].str.strip().isin([\"duplicate_trial\"])]\n",
    "df.to_json('dfs/deco/Intervention_Decoding_df.json') #######\n",
    "print(\"Total number of rows in df  before removal: \", len(df))\n",
    "\n",
    "# create a df with all the rows that have been removed for training\n",
    "df = df[~df['notes'].str.strip().isin([\"not_doing_task\", \"inaudible\", \"not_doing_task inaudible\", \"duplicate_trial\"])]\n",
    "df = df[~df['human_transcription'].str.contains(\"adult\")]\n",
    "df = df[~df['words_human_transcription'].str.strip().isin([\"\"])]\n",
    "df.to_json('dfs/deco/train/Intervention_Decoding_df.json')\n",
    "print(\"Total number of rows in df  before removal: \", len(df))\n",
    "\n",
    "\n",
    "df_removed = df_not_cleaned[~df_not_cleaned.index.isin(df.index)]\n",
    "df_removed = df_removed[~df_removed['notes'].str.strip().isin([\"duplicate_trial\"])]\n",
    "df_removed.to_json('dfs/deco/Intervention_Decoding_removed_df.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05eed43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0033047",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb24321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type of accuracy column of first row\n",
    "print(\"Type of accuracy column of first row: \", type(df.loc[0]['accuracy']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
