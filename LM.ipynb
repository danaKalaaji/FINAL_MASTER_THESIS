{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from transformers import AutoProcessor\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "from transformers import Wav2Vec2ProcessorWithLM\n",
    "from huggingface_hub import Repository,HfApi, snapshot_download\n",
    "from datasets import load_dataset\n",
    "from itertools import islice\n",
    "import random\n",
    "from itertools import islice\n",
    "from datasets import load_dataset\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the text files containing the words to be added to the LM for each language\n",
    "df = pd.read_json(r'dfs/deco/Intervention_df_cleaned_deco.json')\n",
    "df_1 = pd.read_json(r'dfs/deco/Intervention_df_1_deco.json')\n",
    "df_2 = pd.read_json(r'dfs/deco/Intervention_df_2_deco.json')\n",
    "df_3 = pd.read_json(r'dfs/deco/Intervention_df_3_deco.json')\n",
    "df_4 = pd.read_json(r'dfs/deco/Intervention_df_4_deco.json')\n",
    "df_5 = pd.read_json(r'dfs/deco/Intervention_df_5_deco.json')\n",
    "dataframes = [df_1, df_2, df_3, df_4, df_5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the text files for the LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LM containing the words from the reference_text column\n",
    "with open(\"LM/intervention/Fr_Ref.txt\", \"w\") as file:\n",
    "  ref_transcriptions = df[df[\"language\"] == \"fr\"][\"reference_text\"]\n",
    "  file.write(\"\\n\".join(ref_transcriptions))\n",
    "\n",
    "# Remove duplicates words from the LM/intervention/Fr_Ref.txt text files to generate possible mispronounciations\n",
    "with open(\"LM/intervention/Fr_Ref.txt\", \"r\") as input_file:\n",
    "    unique_ref_transcriptions = set(ref_transcriptions)\n",
    "    with open(\"LM/intervention/Fr_Ref_unique.txt\", \"w\") as output_file:\n",
    "        output_file.write(\"\\n\".join(unique_ref_transcriptions))\n",
    "\n",
    "# LM containing the words from the human_transcription column (so with the potential mispronounciations)\n",
    "for i, df_to_exclude in enumerate(dataframes, start=1):\n",
    "    df_no_excluded = pd.concat([df for j, df in enumerate(dataframes) if j != i-1], ignore_index=True)\n",
    "    with open(f\"LM/intervention/Fr_Hum_no_df{i}.txt\", \"w\") as file:\n",
    "        french_transcriptions = df_no_excluded[df_no_excluded[\"language\"] == \"fr\"][\"words_human_transcription\"]\n",
    "        file.write(\"\\n\".join(french_transcriptions))\n",
    "        file.write(\"\\n\".join(ref_transcriptions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'panre', 'poiroi', 'poiré', 'poiren', 'poiru', 'poira', 'penre', 'pure', 'père', 'poiro', 'poirou', 'piore', 'pere', 'poireu', 'ponre', 'poirai', 'poure', 'poiri', 'pare', 'poirè', 'pire', 'pinre', 'pore', 'peure', 'poiran', 'paire', 'poiron', 'pére', 'poirin'}\n"
     ]
    }
   ],
   "source": [
    "# generate the mispronounciations\n",
    "fr_dict_voyelles = {\n",
    "    'a': ['ai', 'e', 'é', 'è', 'i', 'o', 'oi', 'ou', 'u', 'eu', 'on', 'an', 'en', \"in\"],\n",
    "    'ai': ['a', 'e', 'é', 'è', 'i', 'o', 'oi', 'ou', 'u', 'eu', 'on', 'an', 'en', \"in\", \"ia\"],\n",
    "    'aie': ['a', 'e', 'é', 'è', 'i', 'o', 'oi', 'ou', 'u', 'eu', 'on', 'an', 'en', \"in\", \"ia\", \"ae\", \"ei\", \"iea\", \"eai\", \"eia\", \"iae\", \"aei\"],\n",
    "    'au': ['a', 'e', 'é', 'è', 'i', 'oi', 'ou', 'u', 'eu', 'on', 'an', 'en', \"in\", \"ua\"],\n",
    "    'eau': ['a', 'e', 'é', 'è', 'i', 'oi', 'ou', 'u', 'eu', 'on', 'an', 'en', \"in\", \"uae\", \"aeu\", \"aue\", \"uea\", \"eua\", \"ea\", \"ae\", \"ua\", \"ue\"],\n",
    "    'e': ['a', 'ai', 'é', 'è', 'i', 'o', 'oi', 'ou', 'u', 'eu', 'on', 'an', 'en', \"in\"],\n",
    "    'é': ['a', 'ai', 'e', 'è', 'i', 'o', 'oi', 'ou', 'u', 'eu', 'on', 'an', 'en', \"in\"],\n",
    "    'è': ['a', 'ai', 'e', 'é', 'i', 'o', 'oi', 'ou', 'u', 'eu', 'on', 'an', 'en', \"in\"],\n",
    "    'i': ['a', 'ai', 'e', 'é', 'è', 'o', 'oi', 'ou', 'u', 'eu', 'on', 'an', 'en', \"in\"],\n",
    "    'ie': ['a', 'ai', 'e', 'é', 'è', 'i', 'o', 'oi', 'ou', 'u', 'eu', 'on', 'an', 'en', \"in\", \"ei\"],\n",
    "    'o': ['a', 'ai', 'e', 'é', 'è', 'i', 'oi', 'ou', 'u', 'eu', 'on', 'an', 'en', \"in\"],\n",
    "    'oi': ['a', 'ai', 'e', 'é', 'è', 'i', 'o', 'ou', 'u', 'eu', 'on', 'an', 'en', \"in\", \"io\"],\n",
    "    'ou': ['a', 'ai', 'e', 'é', 'è', 'i', 'o', 'oi', 'u', 'eu', 'on', 'an', 'en', \"in\", \"uo\"],\n",
    "    'u': ['a', 'ai', 'e', 'é', 'è', 'i', 'o', 'oi', 'ou', 'eu', 'on', 'an', 'en', \"in\"],\n",
    "    'y': ['a', 'ai', 'e', 'é', 'è', 'o', 'oi', 'ou', 'u', 'eu', 'on', 'an', 'en', \"in\"],\n",
    "    'eu': ['a', 'ai', 'e', 'é', 'è', 'i', 'o', 'oi', 'ou', 'u', 'on', 'an', 'en', \"in\", \"ue\"],\n",
    "    'on': ['a', 'ai', 'e', 'é', 'è', 'i', 'o', 'oi', 'ou', 'u', 'eu', 'an', 'en', \"in\", \"no\", 'one'],\n",
    "    'an': ['a', 'ai', 'e', 'é', 'è', 'i', 'o', 'oi', 'ou', 'u', 'eu', 'on', \"in\", \"na\", \"ane\"],\n",
    "    'en': ['a', 'ai', 'e', 'é', 'è', 'i', 'o', 'oi', 'ou', 'u', 'eu', 'on', \"in\", \"ne\"],\n",
    "    'in': ['a', 'ai', 'e', 'é', 'è', 'i', 'o', 'oi', 'ou', 'u', 'eu', 'on', 'an', \"en\", \"ni\"],\n",
    "    'oeu': ['a', 'ai', 'e', 'é', 'è', 'i', 'o', 'oi', 'ou', 'u', 'on', 'an', 'en', \"in\", \"euo\", \"oue\", \"eou\", \"ueo\", \"uoe\", \"oe\", \"eo\", \"uo\", \"ue\"],\n",
    "    'ê': ['a','e', 'é', 'è', 'i', 'o', 'oi', 'ou', 'u', 'eu', 'on', 'an', 'en', \"in\"],\n",
    "    'ô': ['a','e', 'é', 'è', 'i','oi', 'ou', 'u', 'eu', 'on', 'an', 'en', \"in\"],\n",
    "    'eui': ['a','e', 'é', 'è', 'i', 'o', 'oi', 'ou', 'u', 'eu', 'on', 'an', 'en', \"in\", \"uei\", \"eiu\", \"ieu\",\"iue\", \"eui\", \"ue\", \"ei\", \"ui\", \"iu\"],\n",
    "    'aoû': ['a','e', 'é', 'è', 'i', 'o', 'oi', 'u', 'eu', 'on', 'an', 'en', \"in\", \"ua\", \"oua\", \"auo\", \"aou\", \"oa\", \"ua\", \"uo\"],\n",
    "    'ion': ['a','e', 'é', 'è', 'i', 'o', 'oi', 'ou', 'u', 'eu', 'on', 'an', 'en', \"in\", \"noi\", \"oin\", \"nio\", \"ino\", \"ion\", \"no\", \"oi\", \"ni\", \"io\"],\n",
    "}\n",
    "\n",
    "def generate_mispronunciations(word, dict_voyelles, index=0, mispronunciations=None):\n",
    "    #print(f\"Here we go\")\n",
    "    if mispronunciations is None:\n",
    "        mispronunciations = set()\n",
    "\n",
    "    if index >= len(word):\n",
    "        return mispronunciations\n",
    "    \n",
    "    # Iterate over the word and find the earliest and biggest vowel\n",
    "    earliest_vowel_index = len(word)  # Initialize with a value greater than any possible index\n",
    "    longest_vowel_length = 0\n",
    "    vowel_found = False\n",
    "\n",
    "    #print(f\"Word: {word[index:]} at index {index}\")\n",
    "    for i, c in enumerate(word[index:], start=index):\n",
    "        if vowel_found:\n",
    "            break\n",
    "        for key in sorted(dict_voyelles.keys(), key=len, reverse=True):\n",
    "            if key.startswith(c):\n",
    "                if word[i:i + len(key)] == key:\n",
    "                    if not vowel_found:\n",
    "                        earliest_vowel_index = i\n",
    "                        longest_vowel_length = len(key)\n",
    "                        vowel_found = True\n",
    "                        for value in dict_voyelles[key]:\n",
    "                            new_word = word[:earliest_vowel_index] + value + word[earliest_vowel_index + longest_vowel_length:]\n",
    "                            mispronunciations.add(new_word)\n",
    "                        break\n",
    "        \n",
    "    # If no vowel is found, return the original word\n",
    "    if earliest_vowel_index == len(word):\n",
    "        return mispronunciations\n",
    "    \n",
    "    generate_mispronunciations(word, dict_voyelles, earliest_vowel_index + longest_vowel_length, mispronunciations)\n",
    "    return mispronunciations\n",
    "    \n",
    "    \n",
    "# Example usage\n",
    "word = \"poire\"\n",
    "result = generate_mispronunciations(word, dict_voyelles=fr_dict_voyelles)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"LM/intervention/Fr_Ref_unique.txt\", \"r\") as file:\n",
    "    sentences = file.readlines()\n",
    "all_mispronunciations = {}\n",
    "sentences_misspronounced = []\n",
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    for word in words:\n",
    "        mispronunciations = generate_mispronunciations(word, fr_dict_voyelles)\n",
    "        all_mispronunciations[word] = mispronunciations\n",
    "        for mispronunciation in mispronunciations:\n",
    "            # write the whole sentence with word replaced by the mispronunciation\n",
    "            sentences_misspronounced.append(sentence.replace(word, mispronunciation))\n",
    "# write the sentences with mispronunciations to a file\n",
    "with open(\"LM/intervention/Fr_Ref_mispronounced.txt\", \"w\") as file:\n",
    "    file.write(\"\".join(sentences_misspronounced))\n",
    "\n",
    "for i in range(1, 6):\n",
    "    with open(f\"LM/intervention/Fr_Hum_no_df{i}_vowels.txt\", \"w\") as output_file:\n",
    "        with open(f\"LM/intervention/Fr_Hum_no_df{i}.txt\", \"r\") as input_file:\n",
    "            for line in input_file:\n",
    "                output_file.write(line)\n",
    "        with open(\"LM/intervention/Fr_Ref_mispronounced.txt\", \"r\") as mispronounced_file:\n",
    "            for line in mispronounced_file:\n",
    "                output_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20290\n",
      "1667\n",
      "88450\n",
      "6885\n",
      "108739\n",
      "8551\n"
     ]
    }
   ],
   "source": [
    "# number of words in the \"LM/intervention/Fr_Hum_no_df1.txt\"\n",
    "with open(\"LM/intervention/Fr_Hum_no_df1.txt\", \"r\") as file:\n",
    "    words = file.read().split()\n",
    "    print(len(words))\n",
    "    # number of lines\n",
    "    print(len(open(\"LM/intervention/Fr_Hum_no_df1.txt\").readlines(  )))\n",
    "\n",
    "with open(\"LM/intervention/Fr_Ref_mispronounced.txt\", \"r\") as file:\n",
    "    words = file.read().split()\n",
    "    print(len(words))\n",
    "    # number of lines\n",
    "    print(len(open(\"LM/intervention/Fr_Ref_mispronounced.txt\").readlines(  )))\n",
    "    \n",
    "with open(\"LM/intervention/Fr_Hum_no_df1_vowels.txt\", \"r\") as file:\n",
    "    words = file.read().split()\n",
    "    print(len(words))\n",
    "    # number of lines\n",
    "    print(len(open(\"LM/intervention/Fr_Hum_no_df1_vowels.txt\").readlines(  )))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the LM with KenLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/hnp_vr/ASR_Dana/Master-Thesis-Pipeline/LM/intervention/Fr_Hum_no_df4.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 20278 types 2507\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:30084 2:26788528128\n",
      "Statistics:\n",
      "1 2507 D1=0.805085 D2=1.12841 D3+=0.633245\n",
      "2 5044 D1=0.89506 D2=1.01403 D3+=1.59076\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 152 assuming -p 1.5\n",
      "probing 162 assuming -r models -p 1.5\n",
      "trie     85 without quantization\n",
      "trie     72 assuming -q 8 -b 8 quantization \n",
      "trie     85 assuming -a 22 array pointer compression\n",
      "trie     72 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:30084 2:80704\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:30084 2:80704\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:26316296 kB\tVmRSS:5912 kB\tRSSMax:7143200 kB\tuser:0.174467\tsys:1.0565\tCPU:1.23097\treal:1.23772\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/hnp_vr/ASR_Dana/Master-Thesis-Pipeline/LM/intervention/Fr_Hum_no_df2.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 20364 types 2609\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:31308 2:26788526080\n",
      "Statistics:\n",
      "1 2609 D1=0.811688 D2=1.10784 D3+=0.708174\n",
      "2 5213 D1=0.884985 D2=1.29322 D3+=1.00311\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 157 assuming -p 1.5\n",
      "probing 168 assuming -r models -p 1.5\n",
      "trie     88 without quantization\n",
      "trie     74 assuming -q 8 -b 8 quantization \n",
      "trie     88 assuming -a 22 array pointer compression\n",
      "trie     74 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:31308 2:83408\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:31308 2:83408\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:26308100 kB\tVmRSS:5860 kB\tRSSMax:7143704 kB\tuser:0.159652\tsys:0.63861\tCPU:0.79827\treal:0.806936\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/hnp_vr/ASR_Dana/Master-Thesis-Pipeline/LM/intervention/Fr_Hum_no_df5_vowels.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 108877 types 9589\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:115068 2:26788442112\n",
      "Statistics:\n",
      "1 9589 D1=0.917024 D2=1.29318 D3+=1.56153\n",
      "2 19736 D1=0.95719 D2=1.33995 D3+=1.14483\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 590 assuming -p 1.5\n",
      "probing 627 assuming -r models -p 1.5\n",
      "trie    333 without quantization\n",
      "trie    278 assuming -q 8 -b 8 quantization \n",
      "trie    333 assuming -a 22 array pointer compression\n",
      "trie    278 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:115068 2:315776\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:115068 2:315776\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:26316956 kB\tVmRSS:6552 kB\tRSSMax:7155068 kB\tuser:0.13967\tsys:0.758212\tCPU:0.897893\treal:0.90618\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/hnp_vr/ASR_Dana/Master-Thesis-Pipeline/LM/intervention/Fr_Hum_no_df1.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 20290 types 2620\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:31440 2:26788526080\n",
      "Statistics:\n",
      "1 2620 D1=0.816707 D2=1.08934 D3+=0.744333\n",
      "2 5201 D1=0.888321 D2=1.23054 D3+=1.87335\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 157 assuming -p 1.5\n",
      "probing 168 assuming -r models -p 1.5\n",
      "trie     88 without quantization\n",
      "trie     75 assuming -q 8 -b 8 quantization \n",
      "trie     88 assuming -a 22 array pointer compression\n",
      "trie     75 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:31440 2:83216\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:31440 2:83216\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:26316296 kB\tVmRSS:6096 kB\tRSSMax:7141728 kB\tuser:0.160303\tsys:0.621175\tCPU:0.781488\treal:0.790991\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/hnp_vr/ASR_Dana/Master-Thesis-Pipeline/LM/intervention/Fr_Hum_no_df1_vowels.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 108739 types 9645\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:115740 2:26788442112\n",
      "Statistics:\n",
      "1 9645 D1=0.918287 D2=1.285 D3+=1.37949\n",
      "2 19767 D1=0.95561 D2=1.36655 D3+=1.73899\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 592 assuming -p 1.5\n",
      "probing 630 assuming -r models -p 1.5\n",
      "trie    334 without quantization\n",
      "trie    280 assuming -q 8 -b 8 quantization \n",
      "trie    334 assuming -a 22 array pointer compression\n",
      "trie    280 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:115740 2:316272\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:115740 2:316272\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:26316956 kB\tVmRSS:6316 kB\tRSSMax:7155116 kB\tuser:0.281505\tsys:0.583118\tCPU:0.864637\treal:0.865113\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/hnp_vr/ASR_Dana/Master-Thesis-Pipeline/LM/intervention/Fr_Hum_no_df3_vowels.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 108837 types 9627\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:115524 2:26788442112\n",
      "Statistics:\n",
      "1 9627 D1=0.915748 D2=1.29453 D3+=1.30939\n",
      "2 19776 D1=0.9567 D2=1.36965 D3+=1.20873\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 592 assuming -p 1.5\n",
      "probing 629 assuming -r models -p 1.5\n",
      "trie    334 without quantization\n",
      "trie    279 assuming -q 8 -b 8 quantization \n",
      "trie    334 assuming -a 22 array pointer compression\n",
      "trie    279 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:115524 2:316416\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:115524 2:316416\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:26316956 kB\tVmRSS:6648 kB\tRSSMax:7155464 kB\tuser:0.281001\tsys:0.511824\tCPU:0.792839\treal:0.802954\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/hnp_vr/ASR_Dana/Master-Thesis-Pipeline/LM/intervention/Fr_Hum_no_df3.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 20388 types 2597\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:31164 2:26788526080\n",
      "Statistics:\n",
      "1 2597 D1=0.805295 D2=1.09025 D3+=1.13887\n",
      "2 5209 D1=0.889414 D2=1.20238 D3+=1.51764\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 157 assuming -p 1.5\n",
      "probing 167 assuming -r models -p 1.5\n",
      "trie     88 without quantization\n",
      "trie     74 assuming -q 8 -b 8 quantization \n",
      "trie     88 assuming -a 22 array pointer compression\n",
      "trie     74 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:31164 2:83344\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:31164 2:83344\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:26316296 kB\tVmRSS:5952 kB\tRSSMax:7145700 kB\tuser:0.242411\tsys:0.535325\tCPU:0.777743\treal:0.779911\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/hnp_vr/ASR_Dana/Master-Thesis-Pipeline/LM/intervention/Fr_Ref_mispronounced.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 88450 types 7505\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:90060 2:26788466688\n",
      "/home/hnp_vr/kenlm/lm/builder/adjust_counts.cc:49 in void lm::builder::{anonymous}::StatCollector::CalculateDiscounts(const lm::builder::DiscountConfig&) threw BadDiscountException because `s.n[j] == 0'.\n",
      "Could not calculate Kneser-Ney discounts for 2-grams with adjusted count 3 because we didn't observe any 2-grams with adjusted count 2; Is this small or artificial data?\n",
      "Try deduplicating the input.  To override this error for e.g. a class-based model, rerun with --discount_fallback\n",
      "\n",
      "Aborted\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/hnp_vr/ASR_Dana/Master-Thesis-Pipeline/LM/intervention/Fr_Ref_unique.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 216 types 219\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:2628 2:26788554752\n",
      "/home/hnp_vr/kenlm/lm/builder/adjust_counts.cc:49 in void lm::builder::{anonymous}::StatCollector::CalculateDiscounts(const lm::builder::DiscountConfig&) threw BadDiscountException because `s.n[j] == 0'.\n",
      "Could not calculate Kneser-Ney discounts for 1-grams with adjusted count 3 because we didn't observe any 1-grams with adjusted count 2; Is this small or artificial data?\n",
      "Try deduplicating the input.  To override this error for e.g. a class-based model, rerun with --discount_fallback\n",
      "\n",
      "Aborted\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/hnp_vr/ASR_Dana/Master-Thesis-Pipeline/LM/intervention/Fr_Ref.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 11160 types 219\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:2628 2:26788554752\n",
      "/home/hnp_vr/kenlm/lm/builder/adjust_counts.cc:49 in void lm::builder::{anonymous}::StatCollector::CalculateDiscounts(const lm::builder::DiscountConfig&) threw BadDiscountException because `s.n[j] == 0'.\n",
      "Could not calculate Kneser-Ney discounts for 1-grams with adjusted count 3 because we didn't observe any 1-grams with adjusted count 2; Is this small or artificial data?\n",
      "Try deduplicating the input.  To override this error for e.g. a class-based model, rerun with --discount_fallback\n",
      "\n",
      "Aborted\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/hnp_vr/ASR_Dana/Master-Thesis-Pipeline/LM/intervention/Fr_Hum_no_df2_vowels.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 108813 types 9644\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:115728 2:26788442112\n",
      "Statistics:\n",
      "1 9644 D1=0.917697 D2=1.29087 D3+=1.09263\n",
      "2 19785 D1=0.956405 D2=1.32763 D3+=1.44703\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 592 assuming -p 1.5\n",
      "probing 630 assuming -r models -p 1.5\n",
      "trie    334 without quantization\n",
      "trie    280 assuming -q 8 -b 8 quantization \n",
      "trie    334 assuming -a 22 array pointer compression\n",
      "trie    280 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:115728 2:316560\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:115728 2:316560\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:26325152 kB\tVmRSS:6284 kB\tRSSMax:7153276 kB\tuser:0.200581\tsys:0.742152\tCPU:0.942742\treal:0.944832\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/hnp_vr/ASR_Dana/Master-Thesis-Pipeline/LM/intervention/Fr_Hum_no_df4_vowels.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 108727 types 9556\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:114672 2:26788442112\n",
      "Statistics:\n",
      "1 9556 D1=0.917794 D2=1.34677 D3+=0.986775\n",
      "2 19632 D1=0.960848 D2=1.17964 D3+=1.44854\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 587 assuming -p 1.5\n",
      "probing 625 assuming -r models -p 1.5\n",
      "trie    331 without quantization\n",
      "trie    277 assuming -q 8 -b 8 quantization \n",
      "trie    331 assuming -a 22 array pointer compression\n",
      "trie    277 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:114672 2:314112\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:114672 2:314112\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:26325152 kB\tVmRSS:6364 kB\tRSSMax:7153088 kB\tuser:0.208287\tsys:0.565352\tCPU:0.773649\treal:0.783362\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/hnp_vr/ASR_Dana/Master-Thesis-Pipeline/LM/intervention/Fr_Hum_no_df5.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 20428 types 2562\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:30744 2:26788526080\n",
      "Statistics:\n",
      "1 2562 D1=0.807119 D2=1.05432 D3+=1.26159\n",
      "2 5176 D1=0.890895 D2=1.23221 D3+=1.19566\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 156 assuming -p 1.5\n",
      "probing 166 assuming -r models -p 1.5\n",
      "trie     87 without quantization\n",
      "trie     73 assuming -q 8 -b 8 quantization \n",
      "trie     87 assuming -a 22 array pointer compression\n",
      "trie     73 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:30744 2:82816\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:30744 2:82816\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:26316296 kB\tVmRSS:5928 kB\tRSSMax:7143424 kB\tuser:0.193\tsys:0.619634\tCPU:0.812642\treal:0.814502\n"
     ]
    }
   ],
   "source": [
    "# kenlm/build/bin/lmplz -o number_n_of_n_gram <\"text_file_path\" > \"LM_file_path\"\n",
    "# kenlm/build/bin/lmplz -o 2 <\"ASR_Thesis/It_Ref.txt\" > \"ASR_Thesis/2gram_It_Ref.arpa\"\n",
    "\n",
    "def create_LM(text_file_path, LM_file_path, n, discount_fallback=False):\n",
    "    # remove the empty lines in the text file\n",
    "    with open(text_file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    with open(text_file_path, \"w\") as file:\n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                file.write(line)\n",
    "    command = f\"../../kenlm/build/bin/lmplz -o {n} <{text_file_path}> {LM_file_path}\"\n",
    "    if discount_fallback:\n",
    "        command += \" --discount_fallback\"\n",
    "    os.system(command)\n",
    "\n",
    "    with open(LM_file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    has_added_eos = False\n",
    "    for i, line in enumerate(lines):\n",
    "        if not has_added_eos and \"ngram 1=\" in line:\n",
    "            # Increment the count of unigrams\n",
    "            count = line.strip().split(\"=\")[-1]\n",
    "            lines[i] = line.replace(f\"{count}\", f\"{int(count) + 1}\")\n",
    "        elif not has_added_eos and \"<s>\" in line:\n",
    "            # Write the line and add </s> token\n",
    "            lines.insert(i + 1, line.replace(\"<s>\", \"</s>\"))\n",
    "            has_added_eos = True\n",
    "\n",
    "    with open(LM_file_path, \"w\") as file:\n",
    "        file.writelines(lines)\n",
    "\n",
    "\n",
    "# create_LM for all the files in the LM/intervention directory\n",
    "for file in os.listdir(\"LM/intervention\"):\n",
    "    if file.endswith(\".txt\"):\n",
    "        text_file_path = os.path.join(\"LM/intervention\", file)\n",
    "        LM_file_path = text_file_path.replace(\".txt\", \".arpa\")\n",
    "        create_LM(text_file_path, LM_file_path, 2, discount_fallback=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/hnp_vr/ASR_Dana/Master-Thesis-Pipeline/LM/intervention/Fr_Ref.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 11160 types 219\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:2628 2:9317757952 3:17470797824\n",
      "Substituting fallback discounts for order 0: D1=0.5 D2=1 D3+=1.5\n",
      "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
      "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 219 D1=0.5 D2=1 D3+=1.5\n",
      "2 234 D1=0.5 D2=1 D3+=1.5\n",
      "3 216 D1=0.5 D2=1 D3+=1.5\n",
      "Memory estimate for binary LM:\n",
      "type        B\n",
      "probing 15208 assuming -p 1.5\n",
      "probing 17492 assuming -r models -p 1.5\n",
      "trie     8691 without quantization\n",
      "trie     9766 assuming -q 8 -b 8 quantization \n",
      "trie     8674 assuming -a 22 array pointer compression\n",
      "trie     9749 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:2628 2:3744 3:4320\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:2628 2:3744 3:4320\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:26308040 kB\tVmRSS:5916 kB\tRSSMax:6039484 kB\tuser:0.210288\tsys:0.439694\tCPU:0.649993\treal:0.656343\n"
     ]
    }
   ],
   "source": [
    "# create_LM for ref file with discount_fallback\n",
    "text_file_path = os.path.join(\"LM/intervention/Fr_Ref.txt\")\n",
    "LM_file_path = text_file_path.replace(\".txt\", \".arpa\")\n",
    "create_LM(text_file_path, LM_file_path, 3, discount_fallback=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add LM to a Wav2Vec2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Processing model: jonatasgrosman/wav2vec2-xls-r-1b-french\n",
      "-------------------Processing local_dir: custom_w2v2/xls-r-1/FR_no_LM, lm_model_path: None, repo_id: Dandan0K/Intervention-xls-FR-Hum-no-df1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hnp_vr/miniconda3/envs/asr/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30dbeac5ca14e82b784a082a65b7488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found entries of length > 1 in alphabet. This is unusual unless style is BPE, but the alphabet was not recognized as BPE type. Is this correct?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c1eefa056f47228393486994ba5a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 24 files:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3c7274bd0543eba840226f31c31027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "full_eval.sh:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9249b2ee1040fcbb0e5a9ee1f3e9bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/4.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08323439c41541199dd20064317a1865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "alphabet.json:   0%|          | 0.00/343 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa4ab776bcd479db830885ad49f70ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb87280cefe4bde8ad51df75149f223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval.py:   0%|          | 0.00/6.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131d8332c8e744ecb9d60328b143d244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)voice_8_0_fr_test_predictions_greedy.txt:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f7f23c2db545f2b2adfe16a63f4258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)mon_voice_8_0_fr_test_targets_greedy.txt:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c62443b954435bb08a5a2eeb532846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)2_dev_data_fr_validation_predictions.txt:   0%|          | 0.00/132k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d692b4e6b59341b0aa7011726aa08602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ion_common_voice_8_0_fr_test_targets.txt:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96fb849b54647a5905706335d3677a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "attrs.json:   0%|          | 0.00/78.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0f7a69497f4bb39afdaf706d3ad82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)common_voice_8_0_fr_test_predictions.txt:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73427d473664f83a200df7775a7f165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "lm.binary:   0%|          | 0.00/1.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f61cffd45248ff8a2dc687a8ba449e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ata_fr_validation_predictions_greedy.txt:   0%|          | 0.00/134k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a0349f21b8409b9f8486f4557d9bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unigrams.txt:   0%|          | 0.00/9.81M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb16c01ee5294d8aad360ae9b116f490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ty-v2_dev_data_fr_validation_targets.txt:   0%|          | 0.00/133k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58ffa0d64bd4aa78d629391e324f570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ommon_voice_8_0_fr_test_eval_results.txt:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc7a508b25e4da3a3496c7b1eb100be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)oice_8_0_fr_test_eval_results_greedy.txt:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3135fd89b33d4d48bbb0b7999fdc6463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ta_fr_validation_eval_results_greedy.txt:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773e014465984159a2567eb3b2870146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)_dev_data_fr_validation_eval_results.txt:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing custom_w2v2/xls-r-1/FR_no_LM/language_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hnp_vr/kenlm/util/file.cc:76 in int util::OpenReadOrThrow(const char*) threw ErrnoException because `-1 == (ret = open(name, 00))'.\n",
      "No such file or directory while opening None\n",
      "ERROR\n",
      "rm: cannot remove 'custom_w2v2/xls-r-1/FR_no_LM/language_model/*.arpa': No such file or directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7cbdb20a8854f00ae65925f8ab43b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/3.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------Model jonatasgrosman/wav2vec2-xls-r-1b-french with LM None saved to repo Dandan0K/Intervention-xls-FR-Hum-no-df1\n",
      "-------------------Processing local_dir: custom_w2v2/xls-r-1/Fr_Ref, lm_model_path: LM/intervention/Fr_Ref.arpa, repo_id: Dandan0K/Intervention-xls-FR-Hum-no-df1-vowels\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8b19c5053943989f528c4e8de47b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found entries of length > 1 in alphabet. This is unusual unless style is BPE, but the alphabet was not recognized as BPE type. Is this correct?\n",
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /home/hnp_vr/ASR_Dana/Master-Thesis-Pipeline/LM/intervention/Fr_Ref.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Only 219 unigrams passed as vocabulary. Is this small or artificial data?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c3e00c347c4e16bb084cc9ef94b84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 24 files:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a7b05c396f4ce2a13f92522b1e04ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "attrs.json:   0%|          | 0.00/78.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e49a49231ef4a3a80eff7a52b4389a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "alphabet.json:   0%|          | 0.00/343 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cfd7e335a3843efb41c0995a271dd05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "full_eval.sh:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370dccc0db7946f8896fc9b1f6b0e94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval.py:   0%|          | 0.00/6.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a17d67e6764f2392ef6f189745bfa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe87ca9c840e4eaeb0255abb65893e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "lm.binary:   0%|          | 0.00/1.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d53d35219743d58301df2eec1d5786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)common_voice_8_0_fr_test_predictions.txt:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a56987ce234bee9559f46ad574a1f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/4.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0290418f85464db5add571ae2da12a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ion_common_voice_8_0_fr_test_targets.txt:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df7c3038075492a9f098a1ebcec53c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)voice_8_0_fr_test_predictions_greedy.txt:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec9a394b622476c8be413d5e3ffdf8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)mon_voice_8_0_fr_test_targets_greedy.txt:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e3152bb7434142b6f81e9db035c598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)2_dev_data_fr_validation_predictions.txt:   0%|          | 0.00/132k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f332de24dc05401ab4e14bca050432f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unigrams.txt:   0%|          | 0.00/9.81M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c4e4a843c449aa86ef970c2e09c228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ata_fr_validation_predictions_greedy.txt:   0%|          | 0.00/134k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6669ead74d409c9bdba4534a9ecf09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ty-v2_dev_data_fr_validation_targets.txt:   0%|          | 0.00/133k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0074e7dc48435190acc0c7b8a9d6fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ommon_voice_8_0_fr_test_eval_results.txt:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f631a479662498188a7a5bf58688963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)oice_8_0_fr_test_eval_results_greedy.txt:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8d1ca8151746159388cf5d73f51b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)_dev_data_fr_validation_eval_results.txt:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c7dbf7231740bea4269871ba686399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ta_fr_validation_eval_results_greedy.txt:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing custom_w2v2/xls-r-1/Fr_Ref/language_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading LM/intervention/Fr_Ref.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9f78a231374caaa68f853e1cc1f72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/3.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 70\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# looks like: kenlm/build/bin/build_binary /home/hnp_vr/ASR_Thesis/customWav2vec2/vox/Ref_french/language_model/2gram_Fr_Ref.arpa /home/hnp_vr/ASR_Thesis/customWav2vec2/vox/Ref_french/language_model/2gram_Fr_Ref.bin\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# remove the .arpa file from the language_model folder\u001b[39;00m\n\u001b[1;32m     66\u001b[0m os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrm \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/language_model/*.arpa\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 70\u001b[0m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_folder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprivate_token\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----------------------------------Model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with LM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlm_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m saved to repo \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/asr/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/asr/lib/python3.11/site-packages/huggingface_hub/hf_api.py:1286\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m-> 1286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/asr/lib/python3.11/site-packages/huggingface_hub/hf_api.py:4724\u001b[0m, in \u001b[0;36mHfApi.upload_folder\u001b[0;34m(self, repo_id, folder_path, path_in_repo, commit_message, commit_description, token, repo_type, revision, create_pr, parent_commit, allow_patterns, ignore_patterns, delete_patterns, multi_commits, multi_commits_verbose, run_as_future)\u001b[0m\n\u001b[1;32m   4720\u001b[0m     \u001b[38;5;66;03m# Defining a CommitInfo object is not really relevant in this case\u001b[39;00m\n\u001b[1;32m   4721\u001b[0m     \u001b[38;5;66;03m# Let's return early with pr_url only (as string).\u001b[39;00m\n\u001b[1;32m   4722\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pr_url\n\u001b[0;32m-> 4724\u001b[0m commit_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_commit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4727\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_operations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent_commit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_commit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4734\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4736\u001b[0m \u001b[38;5;66;03m# Create url to uploaded folder (for legacy return value)\u001b[39;00m\n\u001b[1;32m   4737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m create_pr \u001b[38;5;129;01mand\u001b[39;00m commit_info\u001b[38;5;241m.\u001b[39mpr_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/asr/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/asr/lib/python3.11/site-packages/huggingface_hub/hf_api.py:1286\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m-> 1286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/asr/lib/python3.11/site-packages/huggingface_hub/hf_api.py:3677\u001b[0m, in \u001b[0;36mHfApi.create_commit\u001b[0;34m(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads, parent_commit, run_as_future)\u001b[0m\n\u001b[1;32m   3674\u001b[0m \u001b[38;5;66;03m# If updating twice the same file or update then delete a file in a single commit\u001b[39;00m\n\u001b[1;32m   3675\u001b[0m _warn_on_overwriting_operations(operations)\n\u001b[0;32m-> 3677\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreupload_lfs_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3679\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munquoted_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# first-class methods take unquoted revision\u001b[39;49;00m\n\u001b[1;32m   3683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfree_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# do not remove `CommitOperationAdd.path_or_fileobj` on LFS files for \"normal\" users\u001b[39;49;00m\n\u001b[1;32m   3686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3687\u001b[0m files_to_copy \u001b[38;5;241m=\u001b[39m _fetch_files_to_copy(\n\u001b[1;32m   3688\u001b[0m     copies\u001b[38;5;241m=\u001b[39mcopies,\n\u001b[1;32m   3689\u001b[0m     repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3693\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint,\n\u001b[1;32m   3694\u001b[0m )\n\u001b[1;32m   3695\u001b[0m commit_payload \u001b[38;5;241m=\u001b[39m _prepare_commit_payload(\n\u001b[1;32m   3696\u001b[0m     operations\u001b[38;5;241m=\u001b[39moperations,\n\u001b[1;32m   3697\u001b[0m     files_to_copy\u001b[38;5;241m=\u001b[39mfiles_to_copy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3700\u001b[0m     parent_commit\u001b[38;5;241m=\u001b[39mparent_commit,\n\u001b[1;32m   3701\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/asr/lib/python3.11/site-packages/huggingface_hub/hf_api.py:4184\u001b[0m, in \u001b[0;36mHfApi.preupload_lfs_files\u001b[0;34m(self, repo_id, additions, token, repo_type, revision, create_pr, num_threads, free_memory, gitignore_content)\u001b[0m\n\u001b[1;32m   4178\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   4179\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipped upload for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(new_lfs_additions)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(new_lfs_additions_to_upload)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m LFS file(s) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4180\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(ignored by gitignore file).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4181\u001b[0m     )\n\u001b[1;32m   4183\u001b[0m \u001b[38;5;66;03m# Upload new LFS files\u001b[39;00m\n\u001b[0;32m-> 4184\u001b[0m \u001b[43m_upload_lfs_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4185\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_lfs_additions_to_upload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4191\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If `create_pr`, we don't want to check user permission on the revision as users with read permission\u001b[39;49;00m\n\u001b[1;32m   4192\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# should still be able to create PRs even if they don't have write permission on the target branch of the\u001b[39;49;00m\n\u001b[1;32m   4193\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# PR (i.e. `revision`).\u001b[39;49;00m\n\u001b[1;32m   4194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4195\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4196\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m addition \u001b[38;5;129;01min\u001b[39;00m new_lfs_additions_to_upload:\n\u001b[1;32m   4197\u001b[0m     addition\u001b[38;5;241m.\u001b[39m_is_uploaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/asr/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/asr/lib/python3.11/site-packages/huggingface_hub/_commit_api.py:413\u001b[0m, in \u001b[0;36m_upload_lfs_files\u001b[0;34m(additions, repo_type, repo_id, headers, endpoint, num_threads, revision)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(filtered_actions) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    412\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUploading 1 LFS file to the Hub\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 413\u001b[0m     \u001b[43m_wrapped_lfs_upload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_actions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    415\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    416\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUploading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filtered_actions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m LFS files to the Hub using up to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_threads\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m threads concurrently\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/asr/lib/python3.11/site-packages/huggingface_hub/_commit_api.py:403\u001b[0m, in \u001b[0;36m_upload_lfs_files.<locals>._wrapped_lfs_upload\u001b[0;34m(batch_action)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     operation \u001b[38;5;241m=\u001b[39m oid2addop[batch_action[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moid\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m--> 403\u001b[0m     \u001b[43mlfs_upload\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlfs_batch_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while uploading \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation\u001b[38;5;241m.\u001b[39mpath_in_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/asr/lib/python3.11/site-packages/huggingface_hub/lfs.py:243\u001b[0m, in \u001b[0;36mlfs_upload\u001b[0;34m(operation, lfs_batch_action, token, headers, endpoint)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    241\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMalformed response from LFS batch endpoint: `chunk_size` should be an integer. Got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m         )\n\u001b[0;32m--> 243\u001b[0m     \u001b[43m_upload_multi_part\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupload_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupload_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    245\u001b[0m     _upload_single_part(operation\u001b[38;5;241m=\u001b[39moperation, upload_url\u001b[38;5;241m=\u001b[39mupload_url)\n",
      "File \u001b[0;32m~/miniconda3/envs/asr/lib/python3.11/site-packages/huggingface_hub/lfs.py:341\u001b[0m, in \u001b[0;36m_upload_multi_part\u001b[0;34m(operation, header, chunk_size, upload_url)\u001b[0m\n\u001b[1;32m    332\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_transfer is enabled but does not support uploading from bytes or BinaryIO, falling back to regular\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m upload\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    335\u001b[0m     )\n\u001b[1;32m    336\u001b[0m     use_hf_transfer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    338\u001b[0m response_headers \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    339\u001b[0m     _upload_parts_hf_transfer(operation\u001b[38;5;241m=\u001b[39moperation, sorted_parts_urls\u001b[38;5;241m=\u001b[39msorted_parts_urls, chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_hf_transfer\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_upload_parts_iteratively\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorted_parts_urls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msorted_parts_urls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m )\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# 3. Send completion request\u001b[39;00m\n\u001b[1;32m    345\u001b[0m completion_res \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m    346\u001b[0m     upload_url,\n\u001b[1;32m    347\u001b[0m     json\u001b[38;5;241m=\u001b[39m_get_completion_payload(response_headers, operation\u001b[38;5;241m.\u001b[39mupload_info\u001b[38;5;241m.\u001b[39msha256\u001b[38;5;241m.\u001b[39mhex()),\n\u001b[1;32m    348\u001b[0m     headers\u001b[38;5;241m=\u001b[39mLFS_HEADERS,\n\u001b[1;32m    349\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/asr/lib/python3.11/site-packages/huggingface_hub/lfs.py:398\u001b[0m, in \u001b[0;36m_upload_parts_iteratively\u001b[0;34m(operation, sorted_parts_urls, chunk_size)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part_idx, part_upload_url \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sorted_parts_urls):\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m SliceFileObj(\n\u001b[1;32m    393\u001b[0m         fileobj,\n\u001b[1;32m    394\u001b[0m         seek_from\u001b[38;5;241m=\u001b[39mchunk_size \u001b[38;5;241m*\u001b[39m part_idx,\n\u001b[1;32m    395\u001b[0m         read_limit\u001b[38;5;241m=\u001b[39mchunk_size,\n\u001b[1;32m    396\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m fileobj_slice:\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;66;03m# S3 might raise a transient 500 error -> let's retry if that happens\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m         part_upload_res \u001b[38;5;241m=\u001b[39m \u001b[43mhttp_backoff\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPUT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart_upload_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfileobj_slice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_on_status_codes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m502\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m503\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m504\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m         hf_raise_for_status(part_upload_res)\n\u001b[1;32m    402\u001b[0m         headers\u001b[38;5;241m.\u001b[39mappend(part_upload_res\u001b[38;5;241m.\u001b[39mheaders)\n",
      "File \u001b[0;32m~/miniconda3/envs/asr/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:280\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mseek(io_obj_initial_pos)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m retry_on_status_codes:\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/asr/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/envs/asr/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/envs/asr/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:66\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     68\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/miniconda3/envs/asr/lib/python3.11/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/miniconda3/envs/asr/lib/python3.11/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    790\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    809\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/asr/lib/python3.11/site-packages/urllib3/connectionpool.py:496\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBrokenPipeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/asr/lib/python3.11/site-packages/urllib3/connection.py:414\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%x\u001b[39;00m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(chunk), chunk))\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 414\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;66;03m# Regardless of whether we have a body or not, if we're in\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# chunked mode we want to send an explicit empty chunk.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunked:\n",
      "File \u001b[0;32m~/miniconda3/envs/asr/lib/python3.11/http/client.py:1019\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1017\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp.client.send\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, data)\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1019\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msendall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1021\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mIterable):\n",
      "File \u001b[0;32m~/miniconda3/envs/asr/lib/python3.11/ssl.py:1273\u001b[0m, in \u001b[0;36mSSLSocket.sendall\u001b[0;34m(self, data, flags)\u001b[0m\n\u001b[1;32m   1271\u001b[0m         amount \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(byte_view)\n\u001b[1;32m   1272\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m count \u001b[38;5;241m<\u001b[39m amount:\n\u001b[0;32m-> 1273\u001b[0m             v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1274\u001b[0m             count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m v\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/asr/lib/python3.11/ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.send\u001b[0;34m(self, data, flags)\u001b[0m\n\u001b[1;32m   1238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1240\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to send() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1241\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mwrite(data)\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msend(data, flags)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "private_token = \"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\"\n",
    "\n",
    "model_ids = [\"jonatasgrosman/wav2vec2-xls-r-1b-french\"] #\"jonatasgrosman/wav2vec2-xls-r-1b-italian\", \n",
    "\n",
    "local_dirs = [ \"custom_w2v2/xls-r-1/FR_no_LM\", \"custom_w2v2/xls-r-1/Fr_Ref\",\"custom_w2v2/xls-r-1/Fr_Hum_no_df1\", \"custom_w2v2/xls-r-1/Fr_Hum_no_df1_vowels\",\n",
    "             \"custom_w2v2/xls-r-1/Fr_Hum_no_df2\", \"custom_w2v2/xls-r-1/Fr_Hum_no_df2_vowels\", \"custom_w2v2/xls-r-1/Fr_Hum_no_df3\", \"custom_w2v2/xls-r-1/Fr_Hum_no_df3_vowels\",\n",
    "                \"custom_w2v2/xls-r-1/Fr_Hum_no_df4\", \"custom_w2v2/xls-r-1/Fr_Hum_no_df4_vowels\", \"custom_w2v2/xls-r-1/Fr_Hum_no_df5\", \"custom_w2v2/xls-r-1/Fr_Hum_no_df5_vowels\"]\n",
    "\n",
    "lm_model_paths = [None,\"LM/intervention/Fr_Ref.arpa\",  \"LM/intervention/Fr_Hum_no_df1.arpa\", \"LM/intervention/Fr_Hum_no_df1_vowels.arpa\",\n",
    "                 \"LM/intervention/Fr_Hum_no_df2.arpa\", \"LM/intervention/Fr_Hum_no_df2_vowels.arpa\", \"LM/intervention/Fr_Hum_no_df3.arpa\", \"LM/intervention/Fr_Hum_no_df3_vowels.arpa\",\n",
    "                 \"LM/intervention/Fr_Hum_no_df4.arpa\", \"LM/intervention/Fr_Hum_no_df4_vowels.arpa\", \"LM/intervention/Fr_Hum_no_df5.arpa\", \"LM/intervention/Fr_Hum_no_df5_vowels.arpa\"]\n",
    "\n",
    "repo_ids = [ \"Dandan0K/Intervention-xls-FR-no-LM\", \"Dandan0K/Intervention-xls-FR-Ref\", \"Dandan0K/Intervention-xls-FR-Hum-no-df1\", \"Dandan0K/Intervention-xls-FR-Hum-no-df1-vowels\", \n",
    "           \"Dandan0K/Intervention-xls-FR-Hum-no-df2\", \"Dandan0K/Intervention-xls-FR-Hum-no-df2-vowels\", \"Dandan0K/Intervention-xls-FR-Hum-no-df3\", \"Dandan0K/Intervention-xls-FR-Hum-no-df3-vowels\",\n",
    "           \"Dandan0K/Intervention-xls-FR-Hum-no-df4\", \"Dandan0K/Intervention-xls-FR-Hum-no-df4-vowels\", \"Dandan0K/Intervention-xls-FR-Hum-no-df5\", \"Dandan0K/Intervention-xls-FR-Hum-no-df5-vowels\"]\n",
    "\n",
    "local_dirs = [ \"custom_w2v2/xls-r-1/FR_no_LM\", \"custom_w2v2/xls-r-1/Fr_Ref\",\"custom_w2v2/xls-r-1/Fr_Hum_no_df1\", \"custom_w2v2/xls-r-1/Fr_Hum_no_df1_vowels\"]\n",
    "\n",
    "lm_model_paths = [None,\"LM/intervention/Fr_Ref.arpa\",  \"LM/intervention/Fr_Hum_no_df1.arpa\", \"LM/intervention/Fr_Hum_no_df1_vowels.arpa\"]\n",
    "\n",
    "repo_ids = [ \"Dandan0K/Intervention-xls-FR-Hum-no-df1\", \"Dandan0K/Intervention-xls-FR-Hum-no-df1-vowels\", \"Dandan0K/Intervention-xls-FR-Hum-no-df2\", \"Dandan0K/Intervention-xls-FR-Hum-no-df2-vowels\"]\n",
    "\n",
    "\n",
    "for model_id in model_ids:\n",
    "    print(f\"-------------Processing model: {model_id}\")\n",
    "    for local_dir, lm_model_path, repo_id in zip(local_dirs, lm_model_paths, repo_ids):\n",
    "        print(f\"-------------------Processing local_dir: {local_dir}, lm_model_path: {lm_model_path}, repo_id: {repo_id}\")\n",
    "    \n",
    "        api = HfApi()\n",
    "    \n",
    "        api.delete_repo(repo_id= repo_id, token = private_token)\n",
    "        api.create_repo(repo_id= repo_id, private=False, token = private_token)\n",
    "\n",
    "        processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "        vocab_dict = processor.tokenizer.get_vocab()\n",
    "        sorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}\n",
    "\n",
    "        decoder = build_ctcdecoder(\n",
    "            labels=list(sorted_vocab_dict.keys()),\n",
    "            kenlm_model_path = lm_model_path,\n",
    "        )\n",
    "\n",
    "        processor_with_lm = Wav2Vec2ProcessorWithLM(\n",
    "            feature_extractor=processor.feature_extractor,\n",
    "            tokenizer=processor.tokenizer,\n",
    "            decoder=decoder\n",
    "        )\n",
    "\n",
    "        #repo = Repository(local_dir=local_dir, clone_from=model_id)\n",
    "        snapshot_download(repo_id=model_id, local_dir=local_dir, token = private_token, repo_type=\"model\")\n",
    "\n",
    "        # remove the language model folder if it exists\n",
    "        if os.path.exists(f\"{local_dir}/language_model\"):\n",
    "            print(f\"Removing {local_dir}/language_model\")\n",
    "            os.system(f\"rm -r {local_dir}/language_model\")\n",
    "\n",
    "        # Save the model with the new LM\n",
    "        processor_with_lm.save_pretrained(local_dir)\n",
    "\n",
    "        # replace the .arpa file in the language_model folder by the binary file\n",
    "        os.system(f\"../../kenlm/build/bin/build_binary {lm_model_path} {local_dir}/language_model/lm.binary\")\n",
    "        # looks like: kenlm/build/bin/build_binary /home/hnp_vr/ASR_Thesis/customWav2vec2/vox/Ref_french/language_model/2gram_Fr_Ref.arpa /home/hnp_vr/ASR_Thesis/customWav2vec2/vox/Ref_french/language_model/2gram_Fr_Ref.bin\n",
    "\n",
    "        # remove the .arpa file from the language_model folder\n",
    "        os.system(f\"rm {local_dir}/language_model/*.arpa\")\n",
    "\n",
    "\n",
    "\n",
    "        api.upload_folder(\n",
    "            folder_path= local_dir,\n",
    "            repo_id= repo_id,\n",
    "            repo_type=\"model\",\n",
    "            token = private_token\n",
    "        )\n",
    "\n",
    "        print(f\"----------------------------------Model {model_id} with LM {lm_model_path} saved to repo {repo_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
