{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from datasets import Dataset, load_dataset, load_metric, Audio\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "from transformers import TrainingArguments,  Trainer\n",
    "import wandb\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import random\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb9f4db90214ea6896e97de327051b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "private_token = \"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\"\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric, Audio\n",
    "\n",
    "common_voice_train = load_dataset(\"mozilla-foundation/common_voice_6_1\", \"tr\", split=\"train+validation\", use_auth_token=True)\n",
    "common_voice_test = load_dataset(\"mozilla-foundation/common_voice_6_1\", \"tr\", split=\"test\", use_auth_token=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train = common_voice_train.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])\n",
    "common_voice_test = common_voice_test.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])\n",
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    display(HTML(df.to_html()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>seçmen katılımı yüzde seksen civarında gerçekleşti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i̇yi vakit geçirdik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fakat belgradda herkes durumu böyle görmüyor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en az altmış kişi hayatını kaybetti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fikir herkes tarafından hoş karşılanmadı</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ancak zorluklar da yok değil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>polis varlığı barizdi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i̇lk fırın altı aydır atıl durumdaydı</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bosnalı sırp polisi de destek verdi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>temyiz süreci şu anda devam ediyor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b3409567887435b95207bfd2002a460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3478 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175a8b1b3df8463da3096098ab53c15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1647 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "chars_to_remove_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�\\']'\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"sentence\"] = re.sub(chars_to_remove_regex, '', batch[\"sentence\"]).lower()\n",
    "    return batch\n",
    "common_voice_train = common_voice_train.map(remove_special_characters)\n",
    "common_voice_test = common_voice_test.map(remove_special_characters)\n",
    "show_random_elements(common_voice_train.remove_columns([\"path\",\"audio\"]))\n",
    "def replace_hatted_characters(batch):\n",
    "    batch[\"sentence\"] = re.sub('[â]', 'a', batch[\"sentence\"])\n",
    "    batch[\"sentence\"] = re.sub('[î]', 'i', batch[\"sentence\"])\n",
    "    batch[\"sentence\"] = re.sub('[ô]', 'o', batch[\"sentence\"])\n",
    "    batch[\"sentence\"] = re.sub('[û]', 'u', batch[\"sentence\"])\n",
    "    return batch\n",
    "common_voice_train = common_voice_train.map(replace_hatted_characters)\n",
    "common_voice_test = common_voice_test.map(replace_hatted_characters)\n",
    "def extract_all_chars(batch):\n",
    "  all_text = \" \".join(batch[\"sentence\"])\n",
    "  vocab = list(set(all_text))\n",
    "  return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "vocab_train = common_voice_train.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_train.column_names)\n",
    "vocab_test = common_voice_test.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_test.column_names)\n",
    "vocab_list = list(set(vocab_train[\"vocab\"][0]) | set(vocab_test[\"vocab\"][0]))\n",
    "vocab_dict = {v: k for k, v in enumerate(sorted(vocab_list))}\n",
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]\n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "len(vocab_dict)\n",
    "import json\n",
    "with open('vocab.json', 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)\n",
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"./\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "repo_name = \"wav2vec2-large-xls-r-300m-tr-colab\"\n",
    "tokenizer.push_to_hub(repo_name)\n",
    "\n",
    "common_voice_train[0][\"path\"]\n",
    "common_voice_train[0][\"audio\"]\n",
    "common_voice_train = common_voice_train.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "common_voice_test = common_voice_test.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "common_voice_train[0][\"audio\"]\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1392, 659)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample 0.1 fraction of thw whole dataset\n",
    "common_voice_train = common_voice_train.train_test_split(test_size=0.4)['test']\n",
    "common_voice_test = common_voice_test.train_test_split(test_size=0.4)['test']\n",
    "len(common_voice_train), len(common_voice_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d82b0b7b364ee5a3d12888eeb8b52d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1392 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "080a798568f04bdca51efa4a29e1ddff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/659 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def prepare_dataset(batch):\n",
    "    # Load and resample the audio data\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # Extract the input_values from the loaded audio file\n",
    "    # In our case, the Wav2Vec2Processor only normalizes the data\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "\n",
    "    # Encode the transcriptions to label ids\n",
    "    #with processor.as_target_processor():\n",
    "    batch[\"labels\"] = processor(text =batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "common_voice_train = common_voice_train.map(prepare_dataset, remove_columns=common_voice_train.column_names)\n",
    "common_voice_test = common_voice_test.map(prepare_dataset, remove_columns=common_voice_test.column_names)\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "    \"\"\"\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "    \n",
    "data_collator = DataCollatorCTCWithPadding(processor = processor, padding=True)\n",
    "\n",
    "# \n",
    "\n",
    "\n",
    "\n",
    "wer_metric = load_metric(\"wer\")\n",
    "cer_metric = load_metric(\"cer\")\n",
    "\n",
    "# ref: partir avec toi\n",
    "# hyp: portar amec toi\n",
    "# wer: 2/3 = 0.666  -> focus on words level mistakes\n",
    "# cer: 3/13 = 0.230\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['lm_head.bias', 'lm_head.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/hnp_vr/miniconda3/envs/asr/lib/python3.11/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/hnp_vr/miniconda3/envs/asr/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1290' max='1290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1290/1290 30:42, Epoch 29/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>5.156400</td>\n",
       "      <td>2.980128</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.864900</td>\n",
       "      <td>0.669572</td>\n",
       "      <td>0.665391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.186800</td>\n",
       "      <td>0.639628</td>\n",
       "      <td>0.642164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hnp_vr/miniconda3/envs/asr/lib/python3.11/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/hnp_vr/miniconda3/envs/asr/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/hnp_vr/miniconda3/envs/asr/lib/python3.11/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/hnp_vr/miniconda3/envs/asr/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/hnp_vr/miniconda3/envs/asr/lib/python3.11/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/hnp_vr/miniconda3/envs/asr/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1290, training_loss=1.9351934351662332, metrics={'train_runtime': 1845.8984, 'train_samples_per_second': 22.623, 'train_steps_per_second': 0.699, 'total_flos': 5.319547411991914e+18, 'train_loss': 1.9351934351662332, 'epoch': 29.655172413793103})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-xls-r-300m\", \n",
    "    attention_dropout=0.0,\n",
    "    hidden_dropout=0.0,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.0,\n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer),\n",
    ")\n",
    "model.freeze_feature_encoder()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=repo_name,\n",
    "  report_to='wandb',\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=16,\n",
    "  gradient_accumulation_steps=2,\n",
    "  eval_strategy=\"steps\",\n",
    "  num_train_epochs=30,\n",
    "  gradient_checkpointing=True,\n",
    "  fp16=True,\n",
    "  save_steps=400,\n",
    "  eval_steps=400,\n",
    "  logging_steps=400,\n",
    "  learning_rate=1e-4, \n",
    "  warmup_steps=500,\n",
    "  save_total_limit=2,\n",
    "  push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset= common_voice_train,\n",
    "    eval_dataset= common_voice_test,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total audio duration of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"xls_1b_decoding_fr_decoding\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_json(r'dfs/Intervention_df_cleaned_deco.json')\n",
    "df1 = pd.read_json(r'dfs/Intervention_df_1_deco.json') # test\n",
    "df2 = pd.read_json(r'dfs/Intervention_df_2_deco.json') # validation\n",
    "df3 = pd.read_json(r'dfs/Intervention_df_3_deco.json') # train\n",
    "df4 = pd.read_json(r'dfs/Intervention_df_4_deco.json') # train\n",
    "df5 = pd.read_json(r'dfs/Intervention_df_5_deco.json') # train\n",
    "test_dfs = [df1, df2, df3, df4, df5]\n",
    "\n",
    "# train dataset\n",
    "no_df1 = pd.concat([df3, df4, df5], ignore_index=True)\n",
    "no_df2 = pd.concat([df4, df5, df1], ignore_index=True)\n",
    "no_df3 = pd.concat([df5, df1, df2], ignore_index=True)\n",
    "no_df4 = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "no_df5 = pd.concat([df2, df3, df4], ignore_index=True)\n",
    "train_dfs = [no_df1, no_df2, no_df3, no_df4, no_df5]\n",
    "\n",
    "for df in test_dfs:\n",
    "    df['accuracy'] = df['accuracy'].astype(str)\n",
    "for df in train_dfs:\n",
    "    df['accuracy'] = df['accuracy'].astype(str)\n",
    "    \n",
    "len(test_dfs[1]), len(train_dfs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_test_df = pd.DataFrame()  \n",
    "hp_train_df = pd.DataFrame()  \n",
    "\n",
    "for config_id in test_dfs[1]['config_id'].unique():\n",
    "    # if config_id does not contain '_C_' \n",
    "    if not re.search(r'_C_', config_id) and not re.search(r'phondel', config_id):\n",
    "        # Filter DataFrame for the current config_id\n",
    "        config_df_test = test_dfs[1][test_dfs[1]['config_id'] == config_id]\n",
    "        config_df_train = train_dfs[0][train_dfs[0]['config_id'] == config_id]\n",
    "        \n",
    "        # Sample 1/5 of the data and add to hp_test_df and hp_train_df\n",
    "        hp_test_df = pd.concat([hp_test_df, config_df_test.sample(frac=0.2)])\n",
    "        hp_train_df = pd.concat([hp_train_df, config_df_train.sample(frac=0.2)])\n",
    "\n",
    "# Append the new DataFrames to the lists\n",
    "test_dfs.append(hp_test_df)\n",
    "train_dfs.append(hp_train_df)\n",
    "\n",
    "# Print the number of samples per config_id in the new DataFrames\n",
    "print(\"Number of samples in hp_test_df:\", len(test_dfs[-1]))\n",
    "print(\"Number of samples in hp_train_df:\", len(train_dfs[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the vocab.json file to load the vocabulary into an instance of the Wav2Vec2CTCTokenizer class\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"./\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "\n",
    "#tokenizer.push_to_hub(repo_name)\n",
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Audio Datasetfor all the dataframes in the lists\n",
    "train_datasets = []\n",
    "test_datasets = []\n",
    "for i in range(len(test_dfs)):\n",
    "    # replace column filename of the dataframe by an audio column with the path to the recordings folder\n",
    "    train_dfs[i]['audio'] = train_dfs[i]['filepath']\n",
    "    test_dfs[i]['audio'] = test_dfs[i]['filepath']\n",
    "    train_datasets.append(Dataset.from_pandas(train_dfs[i]).cast_column(\"audio\", Audio()))\n",
    "    test_datasets.append(Dataset.from_pandas(test_dfs[i]).cast_column(\"audio\", Audio()))\n",
    "\n",
    "# cast_column(\"audio\", Audio(sampling_rate=16_000)) for all the datasets\n",
    "train_datasets = [dataset.cast_column(\"audio\", Audio(sampling_rate=16_000)) for dataset in train_datasets]\n",
    "test_datasets = [dataset.cast_column(\"audio\", Audio(sampling_rate=16_000)) for dataset in test_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_int = random.randint(0, len(train_datasets[0])-1)\n",
    "print(\"Random Integer:, rand_int\")\n",
    "print(\"Reference Text:\", train_datasets[0][rand_int][\"reference_text\"])\n",
    "print(\"Transcript:\", train_datasets[0][rand_int][\"words_human_transcription\"])\n",
    "print(\"Annotated:\", train_datasets[0][rand_int][\"human_transcription\"])\n",
    "print(\"Input array shape:\", train_datasets[0][rand_int][\"audio\"][\"array\"].shape)\n",
    "print(\"Accuracy:\", train_datasets[0][rand_int][\"accuracy\"])\n",
    "print(\"Sampling rate:\", train_datasets[0][rand_int][\"audio\"][\"sampling_rate\"])\n",
    "ipd.Audio(data=train_datasets[0][rand_int][\"audio\"][\"array\"], autoplay=True, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_datasets[0] \n",
    "test_dataset = test_datasets[1]\n",
    "hp_train_df = train_datasets[-1]\n",
    "hp_test_df = test_datasets[-1]\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    # Load and resample the audio data\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # Extract the input_values from the loaded audio file\n",
    "    # In our case, the Wav2Vec2Processor only normalizes the data\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "\n",
    "    # Encode the transcriptions to label ids\n",
    "    #with processor.as_target_processor():\n",
    "    batch[\"labels\"] = processor(text =batch[\"words_human_transcription\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "#train_dataset = train_dataset.map(prepare_dataset, remove_columns=train_dataset.column_names)\n",
    "#test_dataset = test_dataset.map(prepare_dataset, remove_columns=test_dataset.column_names)\n",
    "hp_train_dataset = hp_train_df.map(prepare_dataset, remove_columns=hp_train_df.column_names)\n",
    "hp_test_dataset = hp_test_df.map(prepare_dataset, remove_columns=hp_test_df.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_train_dataset[10]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "    \"\"\"\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "    \n",
    "data_collator = DataCollatorCTCWithPadding(processor = processor, padding=True)\n",
    "\n",
    "# \n",
    "\n",
    "\n",
    "\n",
    "wer_metric = load_metric(\"wer\")\n",
    "cer_metric = load_metric(\"cer\")\n",
    "\n",
    "# ref: partir avec toi\n",
    "# hyp: portar amec toi\n",
    "# wer: 2/3 = 0.666  -> focus on words level mistakes\n",
    "# cer: 3/13 = 0.230\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    hp_train_dataset,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    hp_test_dataset,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show test_dataloader\n",
    "for batch in test_dataloader:\n",
    "    a = batch['labels']\n",
    "    print(a.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(a)):\n",
    "    print(a[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\n",
    "        \"facebook/wav2vec2-xls-r-300m\", \n",
    "        attention_dropout=0.0,\n",
    "        hidden_dropout=0.0,\n",
    "        feat_proj_dropout=0.0,\n",
    "        mask_time_prob=0.05,\n",
    "        layerdrop=0.0,\n",
    "        ctc_loss_reduction=\"mean\", \n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        vocab_size=len(processor.tokenizer),\n",
    "    )\n",
    "    model.freeze_feature_encoder()\n",
    "    return model\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=repo_name,\n",
    "  report_to='tensorboard',\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=16,\n",
    "  gradient_accumulation_steps=2,\n",
    "  eval_strategy=\"steps\",\n",
    "  num_train_epochs=30,\n",
    "  gradient_checkpointing=True,\n",
    "  fp16=True,\n",
    "  save_steps=400,\n",
    "  eval_steps=400,\n",
    "  logging_steps=400,\n",
    "  learning_rate=3e-4, \n",
    "  warmup_steps=500,\n",
    "  save_total_limit=2,\n",
    "  push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer_hp_search = Trainer(\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset= hp_train_dataset,\n",
    "    eval_dataset= hp_test_dataset,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    model_init=model_init,\n",
    ")\n",
    "\n",
    "def hp_space_optuna(trial) -> Dict[str, float]:\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "        \"weight_decay\": trial.suggest_categorical(\"weight_decay\", [0, 0.1, 0.2, 0.3, 0.4]),\n",
    "    }\n",
    "\n",
    "def hp_space_optuna(trial) -> Dict[str, float]:\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 3e-4, 3e-3, log=True),\n",
    "        #\"weight_decay\": trial.suggest_categorical(\"weight_decay\", [0, 0.1, 0.2, 0.3, 0.4]),\n",
    "    }\n",
    "\n",
    "wandb.require(\"core\")\n",
    "wandb.login(key = '2872d9043dfc3fbb2e6fb6bc3227242d42039f4c')\n",
    "%env WANDB_PROJECT= xls_300m_french_data\n",
    "%env WANDB_LOG_MODEL=true\n",
    "\n",
    "\"\"\"\n",
    "best_trial = trainer_hp_search.hyperparameter_search(\n",
    "    backend=\"optuna\", \n",
    "    n_trials= 1,\n",
    "    hp_space= hp_space_optuna,\n",
    "    study_name=\"xls_300m_french_data_hp_search\",\n",
    "    storage=\"sqlite:///xls_300m_french_data_hp_search.sqlite\",\n",
    "    load_if_exists=True,\n",
    ")\n",
    "\"\"\"\n",
    "\t\n",
    "\n",
    "# Code to delete the study if needed\n",
    "#optuna.delete_study(\"xls_300m_french_data_hp_search\", storage=\"sqlite:///xls_300m_french_data_hp_search.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hp_search = pd.DataFrame(columns=['run_name', 'learning_rate', 'weight_decay', 'wer', 'eval_loss'])\n",
    "\n",
    "# Function to add a line to the DataFrame\n",
    "def add_line(run_name, learning_rate, weight_decay, wer, eval_loss):\n",
    "    global df_hp_search\n",
    "    new_row = pd.DataFrame({'run_name': [run_name], 'learning_rate': [learning_rate], 'weight_decay': [weight_decay], 'wer': [wer], 'eval_loss': [eval_loss]})\n",
    "    df_hp_search = pd.concat([df_hp_search, new_row], ignore_index=True)\n",
    "\n",
    "# Adding lines to the DataFrame\n",
    "add_line('lyric-snow-47', 4.868577622399677e-05, 0.4, 1, 8.02138)\n",
    "add_line('light-vortex-48', 4.736738824344386e-05, 0.1, 1, 9.28767)\n",
    "add_line('dry-monkey-49', 1.7634324378027495e-05, 0, 1, 19.45466)\n",
    "add_line('smooth-sun-50', 2.959961553025127e-05, 0.2, 1, 11.964)\n",
    "add_line('sparkling-cosmos-55', 1.4949795713145915e-05, 0.1, 1, 25.94904)\n",
    "add_line('bright-wind-56', 8.290942488870558e-06, 0, 1, 19.10446)\n",
    "add_line('neat-spaceship-57', 6.331387820954833e-05, 0.4, 1, 4.75191)\n",
    "add_line('devoted-darkness-58', 7.558367556021396e-05, 0.1, 1, 7.13912)\n",
    "add_line('bright-field-59', 8.068243496266396e-06, 0.2, 1, 18.93284)\n",
    "add_line('fiery-wind-60', 1.2457077662674244e-06, 0.3, 1, 47.71626)\n",
    "add_line('robust-jazz-61', 2.6204320199466196e-06, 0.4, 1, 40.64578)\n",
    "add_line('leafy-star-62', 4.2508511104031826e-05, 0.1, 1, 8.57994)\n",
    "add_line('sandy-vortex-63', 9.484154202319027e-05, 0.4, 1, 3.30598)\n",
    "add_line('scarlet-flower-64', 2.9178204397897334e-05, 0.3, 1, 11.13607)\n",
    "add_line('sunny-firefly-65', 4.369441420530818e-06, 0.4, 1, 26.44193)\n",
    "add_line('fancy-totem-66', 4.108700256801521e-05, 0.1, 1, 23.02015)\n",
    "add_line('efficient-planet-67', 1.756418377889514e-05, 0.4, 1, 23.02015)\n",
    "add_line('cool-voice-68', 5.028245035225757e-05, 0.1, 1, 5.85481)\n",
    "add_line('fast-feather-69', 2.5449330568559e-05, 0.2, 1, 17.94265)\n",
    "add_line('decent-lion-70', 9.8941756468773e-05, 0.3, 1, 3.2602896690368652)\n",
    "add_line('dulcet-eon-74', 0.0007462126046290343, 0, 0.9966216216216216, 11.56625)\n",
    "add_line('noble-dawn-75', 0.000995445448672507, 0, 0.9966216216216216, 10.90096950531006)\n",
    "add_line('glowing-fog-76', 0.000969871441864426, 0, 0.9966216216216216, 13.064887046813965)\n",
    "add_line('runs/Jul08_15-17-24_L-1027100151', 0.0007316578851917543, 0, 0, 16.7197)\n",
    "\n",
    "df_hp_search = df_hp_search.sort_values(by='eval_loss').reset_index(drop=True)\n",
    "print(df_hp_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize eval_loss for better color differentiation\n",
    "norm = Normalize(vmin=df_hp_search['eval_loss'].min(), vmax=df_hp_search['eval_loss'].max())\n",
    "\n",
    "plt.figure(figsize=(12, 2))\n",
    "scatter = plt.scatter(\n",
    "    data=df_hp_search,\n",
    "    x='learning_rate',\n",
    "    y='weight_decay',\n",
    "    c='eval_loss',\n",
    "    cmap='plasma',  \n",
    "    s=50,\n",
    "    alpha=0.9,\n",
    "    norm=norm\n",
    ")\n",
    "plt.xscale('log')  # logarithmic scale for learning_rate\n",
    "plt.colorbar(scatter, label='Eval Loss')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Weight Decay')\n",
    "plt.title('Hyperparameter Search')\n",
    "\n",
    "# Annotate points with their index\n",
    "for i, txt in enumerate(df_hp_search.index):\n",
    "    plt.annotate(txt, (df_hp_search['learning_rate'][i], df_hp_search['weight_decay'][i]), fontsize=8, ha='center', va='bottom')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Display top x points\n",
    "x = 15\n",
    "df_hp_search_x = df_hp_search.sort_values(by='eval_loss').reset_index(drop=True)[:x]\n",
    "norm = Normalize(vmin=df_hp_search_x['eval_loss'].min(), vmax=df_hp_search_x['eval_loss'].max())\n",
    "\n",
    "plt.figure(figsize=(12, 2))\n",
    "scatter = plt.scatter(\n",
    "    data=df_hp_search_x,\n",
    "    x='learning_rate',\n",
    "    y='weight_decay',\n",
    "    c='eval_loss',\n",
    "    cmap='viridis',  # Use a diverging colormap for better differentiation\n",
    "    s=50,\n",
    "    alpha=0.9,\n",
    "    norm=norm\n",
    ")\n",
    "plt.xscale('log')  # Use logarithmic scale for learning_rate\n",
    "plt.colorbar(scatter, label='Eval Loss')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Weight Decay')\n",
    "plt.title(f'Hyperparameter Search (Top {x})')\n",
    "\n",
    "# Annotate points with their index\n",
    "for i, txt in enumerate(df_hp_search_x.index):\n",
    "    plt.annotate(txt, (df_hp_search_x['learning_rate'][i], df_hp_search_x['weight_decay'][i]), fontsize=10, ha='center', va='bottom')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-xls-r-300m\", \n",
    "    attention_dropout=0.0,\n",
    "    hidden_dropout=0.0,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.0,\n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer),\n",
    ")\n",
    "model.freeze_feature_encoder()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=repo_name,\n",
    "  report_to='tensorboard',\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=16,\n",
    "  gradient_accumulation_steps=2,\n",
    "  eval_strategy=\"steps\",\n",
    "  num_train_epochs=30,\n",
    "  gradient_checkpointing=True,\n",
    "  fp16=True,\n",
    "  save_steps=400,\n",
    "  eval_steps=400,\n",
    "  logging_steps=400,\n",
    "  learning_rate=1e-4, \n",
    "  warmup_steps=500,\n",
    "  save_total_limit=2,\n",
    "  push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset= train_dataset,\n",
    "    eval_dataset= test_dataset,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push the model to the hub\n",
    "trainer.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "<div>\n",
    "      \n",
    "    <progress value='3150' max='3150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
    "    [3150/3150 11:23:15, Epoch 29/30]\n",
    "  </div>\n",
    "  <table border=\"1\" class=\"dataframe\">\n",
    "<thead>\n",
    "<tr style=\"text-align: left;\">\n",
    "    <th>Step</th>\n",
    "    <th>Training Loss</th>\n",
    "    <th>Validation Loss</th>\n",
    "    <th>Wer</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>400</td>\n",
    "    <td>16.729000</td>\n",
    "    <td>2.744899</td>\n",
    "    <td>1.000000</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>800</td>\n",
    "    <td>2.408700</td>\n",
    "    <td>2.232713</td>\n",
    "    <td>1.005659</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1200</td>\n",
    "    <td>2.245300</td>\n",
    "    <td>2.023961</td>\n",
    "    <td>1.158126</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1600</td>\n",
    "    <td>1.734300</td>\n",
    "    <td>1.468727</td>\n",
    "    <td>1.185791</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2000</td>\n",
    "    <td>1.134500</td>\n",
    "    <td>1.119738</td>\n",
    "    <td>1.104684</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2400</td>\n",
    "    <td>0.795100</td>\n",
    "    <td>1.011836</td>\n",
    "    <td>1.086765</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2800</td>\n",
    "    <td>0.567500</td>\n",
    "    <td>0.841652</td>\n",
    "    <td>1.082678</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table><p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
