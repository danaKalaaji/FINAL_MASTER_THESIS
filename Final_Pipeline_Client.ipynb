{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get an idea of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import torch\n",
    "import concurrent.futures\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor, \n",
    "    Wav2Vec2ForCTC, \n",
    "    Wav2Vec2ProcessorWithLM, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav2vec2(audio, sr, \n",
    "            processor, \n",
    "            model, \n",
    "            pool = None, \n",
    "            num_processes= None,\n",
    "            beam_width= None,\n",
    "            beam_prune_logp = None,\n",
    "            token_min_logp = None,\n",
    "            hotwords = None,\n",
    "            hotword_weight = None,\n",
    "            alpha = None,\n",
    "            beta = None,\n",
    "            unk_score_offset = None,\n",
    "            lm_score_boundary = None,\n",
    "            output_word_offsets= False,\n",
    "            output_char_offsets= False,\n",
    "            phoneme = False,\n",
    "            start_offset = None,\n",
    "            end_offset = None,):\n",
    "    \n",
    "    \n",
    "    start_offset = start_offset if start_offset else 0\n",
    "    end_offset = end_offset if end_offset else len(audio)/sr\n",
    "    audio = audio[int( start_offset * sr ):int( end_offset * sr )]\n",
    "        \n",
    "    inputs = processor(audio, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "    inputs = inputs.to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits.cpu()\n",
    "    \n",
    "    if phoneme:\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription_dict = processor.decode(predicted_ids[0], output_char_offsets = output_char_offsets)\n",
    "        transcription_dict = {\"text\": transcription_dict['text'].lower(), \n",
    "                              \"char_offsets\": transcription_dict['char_offsets'] if output_char_offsets else None }\n",
    "\n",
    "    else:\n",
    "        transcription = processor.batch_decode(logits.numpy(), output_word_offsets=output_word_offsets, \n",
    "                                            pool=pool, num_processes=num_processes, \n",
    "                                            beam_width=beam_width, beam_prune_logp=beam_prune_logp,\n",
    "                                            token_min_logp=token_min_logp, hotwords=hotwords,\n",
    "                                            hotword_weight=hotword_weight, alpha=alpha, beta=beta,\n",
    "                                            unk_score_offset=unk_score_offset, lm_score_boundary=lm_score_boundary)\n",
    "                                            \n",
    "        # compute `time_offset` in seconds as product of downsampling ratio and sampling_rate\n",
    "        time_offset = model.config.inputs_to_logits_ratio / processor.feature_extractor.sampling_rate\n",
    "        for dict in transcription.word_offsets[0]:\n",
    "            dict[\"start_offset\"] = dict[\"start_offset\"] * time_offset\n",
    "            dict[\"end_offset\"] = dict[\"end_offset\"] * time_offset\n",
    "\n",
    "        transcription_dict = {\"text\": transcription.text[0],\n",
    "                            \"word_offsets\": transcription.word_offsets[0] \n",
    "                            }\n",
    "\n",
    "    return transcription_dict, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_speech(word_offsets, target_duration, min_duration, overlap=False):\n",
    "    segments = []\n",
    "    current_segment = []\n",
    "    current_duration = 0.0\n",
    "\n",
    "    if word_offsets[-1]['end_offset'] - word_offsets[0]['start_offset'] < target_duration:\n",
    "        return [{'start_offset': word_offsets[0]['start_offset'], 'end_offset': word_offsets[-1]['end_offset']}]\n",
    "    \n",
    "    for word in word_offsets:\n",
    "        word_duration = word['end_offset'] - word['start_offset']\n",
    "        if current_duration + word_duration > target_duration:\n",
    "            current_segment.append(word)\n",
    "            segments.append(current_segment)\n",
    "            current_segment = [word] if overlap else []\n",
    "            current_duration = word_duration if overlap else 0.0\n",
    "        else:\n",
    "            current_segment.append(word)\n",
    "            current_duration += word_duration\n",
    "\n",
    "    if current_segment:\n",
    "        if current_duration < min_duration and segments:\n",
    "            last_segment = segments.pop()\n",
    "            merged_segment = last_segment + current_segment\n",
    "            split_index = len(merged_segment) // 2\n",
    "            segments.append(merged_segment[:split_index])\n",
    "            segments.append(merged_segment[split_index:])\n",
    "        else:\n",
    "            segments.append(current_segment)\n",
    "\n",
    "    segment_intervals = []\n",
    "    for segment in segments:\n",
    "        start_offset = segment[0]['start_offset']\n",
    "        end_offset = segment[-1]['end_offset']\n",
    "        segment_intervals.append({'start_offset': start_offset, 'end_offset': end_offset})\n",
    "\n",
    "    return segment_intervals\n",
    "\n",
    "\n",
    "# Convert tensors to lists for JSON serialization\n",
    "def convert_tensors_to_lists(obj):\n",
    "    if isinstance(obj, torch.Tensor):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_tensors_to_lists(i) for i in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def convert_lists_to_tensors(obj):\n",
    "    if isinstance(obj, list):\n",
    "        try:\n",
    "            # Try to convert the list to a tensor\n",
    "            return torch.tensor(obj)\n",
    "        except (ValueError, TypeError):\n",
    "            # If the list cannot be converted to a tensor, convert its elements instead\n",
    "            return [convert_lists_to_tensors(i) for i in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "# Columns to be converted from tensors to lists and vice versa\n",
    "tensor_columns = [\n",
    "    'logits_asr',\n",
    "    'logits_phoneme',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models(audio, target_duration_segment, min_duration_segment, overlap, reference_text, processor_phoneme, model_phoneme, processor_asr, model_asr, segmenter = True):\n",
    "\n",
    "    transcriptions_segments = []\n",
    "    char_offsets_segments = []\n",
    "    logits_segments = []\n",
    "    audio_length = librosa.get_duration(y = audio, sr = 16000)\n",
    "\n",
    "    if audio_length < target_duration_segment + min_duration_segment:\n",
    "        segmenter = False\n",
    "\n",
    "    if segmenter:\n",
    "        transcription_asr_segmenter, logits_asr = wav2vec2(audio, 16000, processor=processor_asr, model=model_asr, \n",
    "                                                hotwords= reference_text.split(), output_word_offsets=True, \n",
    "                                                alpha=0.0, beta=0.0)\n",
    "        word_offsets = transcription_asr_segmenter['word_offsets']     \n",
    "        for entry in segment_speech(word_offsets, target_duration_segment, min_duration_segment, overlap):\n",
    "            onset = max(0, entry['start_offset'] - 1)\n",
    "            offset = min(audio_length, entry['end_offset'] + 1)\n",
    "            transcription_clip_all, logits_clip_all = wav2vec2(audio, 16000, processor=processor_phoneme, model=model_phoneme, \n",
    "                                                            start_offset=onset, end_offset=offset, \n",
    "                                                            phoneme=True, output_char_offsets=True)\n",
    "\n",
    "            transcriptions_segments.append(transcription_clip_all['text'])\n",
    "            char_offsets_segments.append(transcription_clip_all['char_offsets'])\n",
    "            logits_segments.append(logits_clip_all)\n",
    "    else:\n",
    "        transcription_clip_all, logits_clip_all = wav2vec2(audio, 16000, processor=processor_phoneme, model=model_phoneme, \n",
    "                                                            phoneme=True, output_char_offsets=True)\n",
    "\n",
    "        transcriptions_segments.append(transcription_clip_all['text'])\n",
    "        char_offsets_segments.append(transcription_clip_all['char_offsets'])\n",
    "        logits_segments.append(logits_clip_all)\n",
    "\n",
    "\n",
    "    return transcriptions_segments, logits_segments, char_offsets_segments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_logit_matrix(logits_segment, char_offsets_segment):\n",
    "    # get offsets of all ' ' characters in the char_offsets_segments dict\n",
    "    space_offsets = [(item['start_offset'], item['end_offset']) for item in char_offsets_segment if item['char'] == ' ']\n",
    "    if not space_offsets:\n",
    "        space_offsets = [(0, len(char_offsets_segment))]\n",
    "\n",
    "    # cut the logits and the prediction at the first space\n",
    "    sub_logits_segments = []\n",
    "    prev_offset = 0\n",
    "    for offset in space_offsets:\n",
    "        # change size of logits_segment0 to the size of the first segment\n",
    "        sub_logits_segment = logits_segment[:, prev_offset:offset[1]]\n",
    "        prev_offset = offset[0]\n",
    "        sub_logits_segments.append(sub_logits_segment)\n",
    "            \n",
    "    return sub_logits_segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_predictions(sub_logits_segment, processor_phoneme, threshold_diff, \n",
    "                         max_pred_per_word, k):\n",
    "    # Get top-k values and indices\n",
    "    topk_values, topk_indices = torch.topk(sub_logits_segment, k=k, dim=-1)\n",
    "    top1_indices = topk_indices[0, :, 0].tolist()\n",
    "\n",
    "    # Initialize with the top-1 prediction\n",
    "    predictions = [tuple(top1_indices)]\n",
    "\n",
    "    # Iterate over each alternative prediction for each timeframe\n",
    "    for j in range(1, k):\n",
    "        for i in range(len(top1_indices)):  \n",
    "\n",
    "            top1_idx = topk_indices[0][i][0].item()\n",
    "            top1_value = topk_values[0][i][0].item()\n",
    "            alt_idx = topk_indices[0][i][j].item()\n",
    "            alt_value = topk_values[0][i][j].item()\n",
    "\n",
    "            # If the top1 prediction is a pad token and the alternative is a space token or vice versa, skip (no need to swap)\n",
    "            if not ((top1_idx == pad_token_id and alt_idx == space_token_id) \n",
    "                    or (top1_idx == space_token_id and alt_idx == pad_token_id)):\n",
    "                \n",
    "                diff = top1_value - alt_value\n",
    "                if diff < threshold_diff:\n",
    "                    new_predictions = []\n",
    "                    for pred in predictions:\n",
    "                        if len(predictions) >= max_pred_per_word + 1:\n",
    "                            break\n",
    "                        new_prediction = list(pred)\n",
    "                        new_prediction[i] = topk_indices[0][i][j]\n",
    "                        new_predictions.append(tuple(new_prediction))\n",
    "                    # Add new predictions and remove duplicates\n",
    "                    predictions.extend(new_predictions)\n",
    "                    predictions = list(set(predictions))\n",
    "\n",
    "            # Stop if we have reached the max prediction count\n",
    "            if len(predictions) > max_pred_per_word:\n",
    "                break\n",
    "\n",
    "    predictions = [torch.tensor(prediction) for prediction in predictions]\n",
    "\n",
    "    # decode the predictions and add them to the all_decoded_predictions list\n",
    "    decoded_predictions = processor_phoneme.batch_decode(predictions)\n",
    "    # remove duplicates\n",
    "    decoded_predictions = list(set(decoded_predictions))\n",
    "    return decoded_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_phonetic_prediction_concat_error(all_decoded_predictions, target_phonemes, error_list, correct, results_decode):\n",
    "    # Binary classification of the reference text\n",
    "    for j in range(1, len(all_decoded_predictions) -1):\n",
    "        previous_pred_list = all_decoded_predictions[j-1]\n",
    "        current_pred_list = all_decoded_predictions[j]\n",
    "        next_pred_list = all_decoded_predictions[j+1]\n",
    "        decoded_prediction_concat = []\n",
    "        for previous_pred in previous_pred_list:\n",
    "            for current_pred in current_pred_list:\n",
    "                for next_pred in next_pred_list:\n",
    "                    concat_pred = previous_pred + current_pred + next_pred\n",
    "                    decoded_prediction_concat.append(concat_pred)\n",
    "\n",
    "        for i, targets_words in enumerate(target_phonemes):\n",
    "            for concat_pred in decoded_prediction_concat:\n",
    "                if any(error in concat_pred for error in error_list[i]):\n",
    "                    correct[i] = 0\n",
    "                    break\n",
    "                for target_word in targets_words:\n",
    "                    if not correct[i] and target_word in concat_pred:\n",
    "                        correct[i] = 1\n",
    "                        results_decode[i] = [concat_pred]\n",
    "                        break\n",
    "\n",
    "    return correct, all_decoded_predictions, results_decode            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_targets(target_excel_path, sheet_name = None):\n",
    "\n",
    "    if sheet_name != None:\n",
    "        df_targets = pd.read_excel(target_excel_path, sheet_name=sheet_name)\n",
    "    else:\n",
    "        df_targets = pd.read_excel(target_excel_path)\n",
    "\n",
    "    # for each element in Target_IPA column, if format is '[element1] or [elements2]' change it to [element1, element2]\n",
    "    df_targets['Target_IPA'] = df_targets['Target_IPA'].apply(lambda x: x.split(\" \") if isinstance(x, str) else [])\n",
    "    df_targets['Asr_IPA_Other'] = df_targets['Asr_IPA_Other'].apply(lambda x: x.split(\" \") if isinstance(x, str) else [])\n",
    "    df_targets['Errors_Letter_End'] = df_targets['Errors_Letter_End'].apply(lambda x: x.split(\" \") if isinstance(x, str) else [])\n",
    "    df_targets['Errors_Letter_Begining'] = df_targets['Errors_Letter_Begining'].apply(lambda x: x.split(\" \") if isinstance(x, str) else [])\n",
    "    # Create a new column for the errors words (empty list)\n",
    "    df_targets['Errors_Words'] = None\n",
    "\n",
    "    # add Asr_IPA_Other to Target_IPA\n",
    "    df_targets['Target_IPA'] = df_targets.apply(lambda x: x['Target_IPA'] + x['Asr_IPA_Other'], axis=1)\n",
    "\n",
    "    # for row, take all words in target_IPA list of that row and add each item (if not empty) of error_letter list to it, then add the result to the errors words column\n",
    "    df_targets['Errors_Letter_End'] = df_targets.apply(\n",
    "        lambda x: [word + letter for word in x['Target_IPA'] for letter in x['Errors_Letter_End'] if letter], axis=1)\n",
    "    df_targets['Errors_Letter_Begining'] = df_targets.apply(\n",
    "        lambda x: [letter + word for word in x['Target_IPA'] for letter in x['Errors_Letter_Begining'] if letter], axis=1)\n",
    "    # for each word in the list in the Target_IPA column, add word\\u0303' to the errors column list\n",
    "    df_targets['Errors_Tilde'] = df_targets.apply(lambda x: [word + '\\u0303' for word in x['Target_IPA']], axis=1)\n",
    "    # add all errors to the errors words column\n",
    "    df_targets['Errors_Words'] = df_targets.apply(\n",
    "        lambda x:   x['Errors_Letter_End'] + x['Errors_Letter_Begining'] + x['Errors_Tilde'], axis=1)\n",
    "\n",
    "    # remove all empty strings from the list\n",
    "    df_targets['Errors_Words'] = df_targets['Errors_Words'].apply(lambda x: [i for i in x if i])\n",
    "    df_targets['Target_IPA'] = df_targets['Target_IPA'].apply(lambda x: [i for i in x if i])\n",
    "\n",
    "    return df_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hnp_vr/miniconda3/envs/asr/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at Dandan0K/Intervention-xls-FR-no-LM were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at Dandan0K/Intervention-xls-FR-no-LM and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b341252ee0643fba3739750629d931d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Cnam-LMSSC/wav2vec2-french-phonemizer were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at Cnam-LMSSC/wav2vec2-french-phonemizer and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_asr_segmenter = Wav2Vec2ForCTC.from_pretrained(\"Dandan0K/Intervention-xls-FR-no-LM\").to(device)\n",
    "processor_asr_segmenter = Wav2Vec2ProcessorWithLM.from_pretrained(\"Dandan0K/Intervention-xls-FR-no-LM\")\n",
    "\n",
    "model_asr_phonemizer = Wav2Vec2ForCTC.from_pretrained(\"Cnam-LMSSC/wav2vec2-french-phonemizer\").to(device)\n",
    "processor_asr_phonemizer = Wav2Vec2Processor.from_pretrained(\"Cnam-LMSSC/wav2vec2-french-phonemizer\")\n",
    "\n",
    "# To be changed if you change the phoneme model\n",
    "token_dict = {\"1\": 1, \"a\": 2, \"b\": 3, \"d\": 4, \"e\": 5, \"f\": 6, \"h\": 7, \"i\": 8, \"j\": 9, \"k\": 10, \n",
    "              \"l\": 11, \"m\": 12, \"n\": 13, \"o\": 14, \"p\": 15, \"r\": 16, \"s\": 17, \"t\": 18, \"u\": 19, \"v\": 20, \n",
    "              \"w\": 21, \"x\": 22, \"y\": 23, \"z\": 24, \"ç\": 25, \"ð\": 26, \"ø\": 27, \"ŋ\": 28, \"œ\": 29, \"ɐ\": 30, \n",
    "              \"ɑ\": 31, \"ɒ\": 32, \"ɔ\": 33, \"ə\": 34, \"ɛ\": 35, \"ɜ\": 36, \"ɡ\": 37, \"ɣ\": 38, \"ɨ\": 39, \"ɪ\": 40, \n",
    "              \"ɬ\": 41, \"ɲ\": 42, \"ɹ\": 43, \"ɾ\": 44, \"ʁ\": 45, \"ʃ\": 46, \"ʊ\": 47, \"ʌ\": 48, \"ʍ\": 49, \"ʒ\": 50, \n",
    "              \"ʔ\": 51, \"ʲ\": 52, \"ː\": 53, \"̃\": 54, \"β\": 55, \"θ\": 56, \"|\": 0, \"[UNK]\": 57, \"[PAD]\": 58}\n",
    "\n",
    "df_targets = process_targets('Targets_IPA.xlsx', sheet_name='Decoding_FR')\n",
    "\n",
    "k = len(token_dict)\n",
    "max_pred_per_word = 20\n",
    "threshold_diff = 2\n",
    "min_duration_speech = 2\n",
    "optimal_duration_speech = 3\n",
    "\n",
    "def get_score(audio_file, target_duration_segment, min_duration_segment, reference_text, processor_phoneme, model_phoneme, processor_asr, model_asr, df_targets, overlap=False, segmenter=True):\n",
    "    \n",
    "    reference_words = reference_text.split(\" \")\n",
    "    target_phonemes = []\n",
    "    error_list = []\n",
    "    correct = [0] * len(reference_words)\n",
    "    results_decode = [[]] * len(reference_words)\n",
    "\n",
    "    for word in reference_words:\n",
    "        target_phonemes.append(df_targets[df_targets['Target'] == word]['Target_IPA'].values[0])\n",
    "        error_list.append(df_targets[df_targets['Target'] == word]['Errors_Words'].values[0])\n",
    "\n",
    "    # Segment the audio file into speech segments and return the logits matrix and character offsets for each segment\n",
    "    _, logits_segments, char_offsets_segments = run_models(audio_file, target_duration_segment, min_duration_segment, overlap, reference_text, processor_phoneme, model_phoneme, processor_asr, model_asr, segmenter)\n",
    "    # Process the logits matrix for each segment\n",
    "    for logits_segment, char_offsets_segment in zip(logits_segments, char_offsets_segments): ### process each segment in parallel\n",
    "\n",
    "        # Split the logits at the first space character and return the sub-logits matrix, target phonemes, and error list\n",
    "        sub_logits_segments = split_logit_matrix(logits_segment, char_offsets_segment)\n",
    "        \n",
    "        # Decode the most probable phonetic predictions for each segment\n",
    "        all_decoded_predictions = []\n",
    "        for sub_logits_segment in sub_logits_segments: ### process each subsegment of each segment in parallel\n",
    "            decoded_predictions = multiple_predictions(sub_logits_segment, processor_phoneme, threshold_diff, max_pred_per_word, k)\n",
    "            all_decoded_predictions.append(decoded_predictions)\n",
    "        \n",
    "        # Binary classification of the reference text\n",
    "        correct, _, results_decode = decode_phonetic_prediction_concat_error(all_decoded_predictions, target_phonemes, error_list, correct, results_decode) ### process each segment in parallel\n",
    "    \n",
    "    return correct, results_decode\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2 0 0 2 0 0 0 0 2 0 2\n",
      "3101_edugame2023_9a4621529c23405c8d2681287fed34a4_eae17d2661f944718181014af9a5c1c8.wav\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'processor_phoneme' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m pad_token_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m58\u001b[39m\n\u001b[1;32m      9\u001b[0m space_token_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 10\u001b[0m get_score(audio, optimal_duration_speech, min_duration_speech, reference_text, \u001b[43mprocessor_phoneme\u001b[49m, model_phoneme, processor_asr, model_asr, df_targets_decoding, overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, segmenter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'processor_phoneme' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"dfs/deco/Intervention_df_cleaned_deco.json\")\n",
    "i = 1\n",
    "filepath = df['filepath'][i]\n",
    "audio, sr = librosa.load(filepath, sr=16000)\n",
    "reference_text = df['reference_text'][i]\n",
    "print(df['accuracy'][i])\n",
    "print(df['filename'][i])\n",
    "pad_token_id = 58\n",
    "space_token_id = 0\n",
    "get_score(audio, optimal_duration_speech, min_duration_speech, reference_text, processor_phoneme, model_phoneme, processor_asr, model_asr, df_targets_decoding, overlap=False, segmenter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 500\n",
      "{\"error\":\"An unexpected error occurred\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import base64\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "import argparse\n",
    "import sys\n",
    "#import sounddevice as sd\n",
    "import numpy as np\n",
    "import threading\n",
    "import queue\n",
    "def audio_to_base64(audio_data, fs=16000):\n",
    "    temp_file = \"temp_audio.wav\"\n",
    "    wav.write(temp_file, fs, audio_data.astype(np.int16))\n",
    "    with open(temp_file, \"rb\") as audio_file:\n",
    "        return base64.b64encode(audio_file.read()).decode('utf-8')\n",
    "    \n",
    "def process_audio(reference_text, audio_data=None, audio_file=None):\n",
    "    if audio_data is None and audio_file is None:\n",
    "        raise ValueError(\"Either audio_data or audio_file must be provided\")\n",
    "    if audio_file:\n",
    "        with open(audio_file, \"rb\") as file:\n",
    "            audio_base64 = base64.b64encode(file.read()).decode('utf-8')\n",
    "    else:\n",
    "        audio_base64 = audio_to_base64(audio_data)\n",
    "\n",
    "    url = \"http://localhost:8070/asr_pipeline\"\n",
    "    payload = {\n",
    "        \"reference_text\": reference_text,\n",
    "        \"audio\": audio_base64\n",
    "    }\n",
    "    response = requests.post(url, json=payload)\n",
    "    if response.status_code == 201:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "audio_data, sr = librosa.load(filepath, sr=16000)\n",
    "reference_text = df['reference_text'][i]\n",
    "process_audio(reference_text, audio_data=audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sounddevice in /home/hnp_vr/miniconda3/envs/asr/lib/python3.11/site-packages (0.5.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in /home/hnp_vr/miniconda3/envs/asr/lib/python3.11/site-packages (from sounddevice) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /home/hnp_vr/miniconda3/envs/asr/lib/python3.11/site-packages (from CFFI>=1.0->sounddevice) (2.22)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sounddevice"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
